<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/83683353ad1dcb56.css" as="style"/><link rel="stylesheet" href="/_next/static/css/83683353ad1dcb56.css" data-n-g=""/><link rel="preload" href="/_next/static/css/1feeaed705173f12.css" as="style"/><link rel="stylesheet" href="/_next/static/css/1feeaed705173f12.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-5866cead997b9ace.js" defer=""></script><script src="/_next/static/chunks/main-f37dcad5231e7164.js" defer=""></script><script src="/_next/static/chunks/pages/_app-53e68f2455fa018c.js" defer=""></script><script src="/_next/static/chunks/278-6fddd24af7f03943.js" defer=""></script><script src="/_next/static/chunks/299-abd0aad0309c30e3.js" defer=""></script><script src="/_next/static/chunks/pages/blogs/pages/posts/%5Bid%5D-757ef647e5f57c03.js" defer=""></script><script src="/_next/static/JnDgV02xcEzLdsb_7ZNVe/_buildManifest.js" defer=""></script><script src="/_next/static/JnDgV02xcEzLdsb_7ZNVe/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main><p class="font-mono text-xl pt-4 text-center font-bold">Writing Custom backward in PyTorch</p><p class="font-mono text-center text-s">2023-08</p><p class="font-mono text-center text-s">Rahul Chand</p><p class="font-mono text-center text-sm">------------</p><div class="Home_markdown___3bxt"><ol>
<li>Custom backward : Two ways to write custom backward in PyTorch.</li>
</ol>
<h3>1. Define a nn.Module with a backward and forward function in a class and use the <em>.backward()</em></h3>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#c792ea">import</span><span> torch
</span><span></span><span class="token token" style="color:#c792ea">class</span><span> </span><span class="token token" style="color:#f2ff00">MyCustomFunction</span><span class="token token" style="color:#89ddff">(</span><span>torch</span><span class="token token" style="color:#89ddff">.</span><span>autograd</span><span class="token token" style="color:#89ddff">.</span><span>Function</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">:</span><span>
</span><span>    </span><span class="token token decorator annotation" style="color:#89ddff">@staticmethod</span><span>
</span><span>    </span><span class="token token" style="color:#c792ea">def</span><span> </span><span class="token token" style="color:#c792ea">forward</span><span class="token token" style="color:#89ddff">(</span><span>ctx</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#ffcb6b">input</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">:</span><span>
</span><span>        </span><span class="token token" style="color:#616161"># Perform forward computation</span><span>
</span><span>        ctx</span><span class="token token" style="color:#89ddff">.</span><span>save_for_backward</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">input</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>        output </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#ffcb6b">input</span><span> </span><span class="token token" style="color:#89ddff">*</span><span> </span><span class="token token" style="color:#fd9170">2</span><span>
</span><span>        </span><span class="token token" style="color:#c792ea">return</span><span> output
</span>
<span>    </span><span class="token token decorator annotation" style="color:#89ddff">@staticmethod</span><span>
</span><span>    </span><span class="token token" style="color:#c792ea">def</span><span> </span><span class="token token" style="color:#c792ea">backward</span><span class="token token" style="color:#89ddff">(</span><span>ctx</span><span class="token token" style="color:#89ddff">,</span><span> grad_output</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">:</span><span>
</span><span>        </span><span class="token token" style="color:#616161"># Perform backward computation</span><span>
</span><span>        </span><span class="token token" style="color:#ffcb6b">input</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> ctx</span><span class="token token" style="color:#89ddff">.</span><span>saved_tensors
</span><span>        grad_input </span><span class="token token" style="color:#89ddff">=</span><span> grad_output </span><span class="token token" style="color:#89ddff">*</span><span> </span><span class="token token" style="color:#fd9170">2</span><span>
</span><span>        </span><span class="token token" style="color:#c792ea">return</span><span> grad_input</span></code></div></pre>
<ul>
<li>We use @staticmethod because no instantiation is required.</li>
<li>ctx is used to save all values that will be used in backward. These will be saved as part of the computation graph</li>
</ul>
<h3>2. Define a class with a normal forward &amp; a backward where you manually set the gradients</h3>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>
</span><span></span><span class="token token" style="color:#c792ea">class</span><span> </span><span class="token token" style="color:#f2ff00">MyCustomFunction</span><span class="token token" style="color:#89ddff">(</span><span>nn</span><span class="token token" style="color:#89ddff">.</span><span>Module</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">:</span><span>
</span><span>	</span><span class="token token" style="color:#c792ea">def</span><span> </span><span class="token token" style="color:#c792ea">__init__</span><span class="token token" style="color:#89ddff">(</span><span>self</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">:</span><span>
</span><span>		self</span><span class="token token" style="color:#89ddff">.</span><span>weight </span><span class="token token" style="color:#89ddff">=</span><span> nn</span><span class="token token" style="color:#89ddff">.</span><span>Linear</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#fd9170">4</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#fd9170">5</span><span class="token token" style="color:#89ddff">)</span><span>
</span>
<span>	</span><span class="token token" style="color:#c792ea">def</span><span> </span><span class="token token" style="color:#c792ea">forward</span><span class="token token" style="color:#89ddff">(</span><span>self</span><span class="token token" style="color:#89ddff">,</span><span> x</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">:</span><span>
</span><span>		</span><span class="token token" style="color:#c792ea">return</span><span> self</span><span class="token token" style="color:#89ddff">.</span><span>weight</span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">)</span><span>
</span>	
<span>	</span><span class="token token" style="color:#c792ea">def</span><span> </span><span class="token token" style="color:#c792ea">backward</span><span class="token token" style="color:#89ddff">(</span><span>self</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#ffcb6b">input</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">:</span><span>
</span><span>		self</span><span class="token token" style="color:#89ddff">.</span><span>weight</span><span class="token token" style="color:#89ddff">.</span><span>grad </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#ffcb6b">input</span><span>
</span><span>		</span><span class="token token" style="color:#c792ea">return</span><span> grad_out
</span>		</code></div></pre>
<ul>
<li>In this case we will call this class inside a <code class="">torch.no_grad()</code> and whatever gradient is returned from it will be used to call the natural <code class="">.backward(grad_out)</code></li>
</ul></div><div class="Home_main3__oLarL"></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"customBackward","contentHtml":"\n1. Custom backward : Two ways to write custom backward in PyTorch. \n\n### 1. Define a nn.Module with a backward and forward function in a class and use the *.backward()*\n\n```python\nimport torch\nclass MyCustomFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input):\n        # Perform forward computation\n        ctx.save_for_backward(input)\n        output = input * 2\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Perform backward computation\n        input, = ctx.saved_tensors\n        grad_input = grad_output * 2\n        return grad_input\n```\n\n- We use @staticmethod because no instantiation is required. \n- ctx is used to save all values that will be used in backward. These will be saved as part of the computation graph\n\n### 2. Define a class with a normal forward \u0026 a backward where you manually set the gradients\n\n```python\n\nclass MyCustomFunction(nn.Module):\n\tdef __init__(self):\n\t\tself.weight = nn.Linear(4,5)\n\n\tdef forward(self, x):\n\t\treturn self.weight(x)\n\t\n\tdef backward(self, input):\n\t\tself.weight.grad = input\n\t\treturn grad_out\n\t\t\n```\n\n- In this case we will call this class inside a `torch.no_grad()` and whatever gradient is returned from it will be used to call the natural `.backward(grad_out)`","title":"Writing Custom backward in PyTorch","date":"2023-08","name":"Rahul Chand"}},"__N_SSG":true},"page":"/blogs/pages/posts/[id]","query":{"id":"customBackward"},"buildId":"JnDgV02xcEzLdsb_7ZNVe","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>