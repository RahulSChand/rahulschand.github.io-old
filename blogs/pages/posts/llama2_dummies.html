<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/93eed7ba81e7490c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/93eed7ba81e7490c.css" data-n-g=""/><link rel="preload" href="/_next/static/css/34a4393968fbda92.css" as="style"/><link rel="stylesheet" href="/_next/static/css/34a4393968fbda92.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-5c046346608af636.js" defer=""></script><script src="/_next/static/chunks/framework-6956cc2a6f4190cb.js" defer=""></script><script src="/_next/static/chunks/main-cb05a03021c19583.js" defer=""></script><script src="/_next/static/chunks/pages/_app-af0621b1a7c9b79f.js" defer=""></script><script src="/_next/static/chunks/278-eceab84b46d12ac0.js" defer=""></script><script src="/_next/static/chunks/299-d50a84aef3cd69a9.js" defer=""></script><script src="/_next/static/chunks/pages/blogs/pages/posts/%5Bid%5D-8d816ebe6e0c60b1.js" defer=""></script><script src="/_next/static/GYLvD-C5QPNQCNXEIuLtF/_buildManifest.js" defer=""></script><script src="/_next/static/GYLvD-C5QPNQCNXEIuLtF/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main><p class="font-mono text-xl pt-4 text-center font-bold">llama2.c for dummies - Introduction to llama2 in C (Taken directly from my github)</p><p class="font-mono text-center text-s">2023-08</p><p class="font-mono text-center text-s">Rahul Chand</p><p class="font-mono text-center text-sm">------------</p><div class="Home_markdown___3bxt"><h1>llama2.c for Dummies</h1>
<h3>Purpose</h3>
<p>This repo is line by line walk through of the inference file in <a href="http://github.com/karpathy/llama2.c">llama2.c</a>. Its very verbose &amp; intended for beginners.</p>
<p>You will need some familiarity with transformers architecture. If you are a complete novice refer to this excellent <a href="https://jalammar.github.io/illustrated-transformer/">blog</a> first.</p>
<hr/>
<h3>Prerequisites</h3>
<ol>
<li>Transformer architecture: 3 components<!-- -->
<ol>
<li>Embedding (1 matmul)</li>
<li>Layers: matmul with Q, K , V, O and feed forward weights: W1, W2 &amp; W3. (7 matmul)</li>
<li>Classifier: In our case the classifier is just matmul of <code class="">(vocab,768) x (768,1)</code> . Basically giving us what is the probability of each next token. (1 matmul)</li>
</ol>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/arch.png" alt="Architechure "/></p>
<h2>Code walkthrough</h2>
<p>Code has 3 parts, structs, functions &amp; read logic in <code class="">main()</code> we will take a look at structs first, then go to main() and then cover the important functions.</p>
<p><strong>PS: The code was taken from commit 4e23ad83. The original repo might be different as it gets newer commits.</strong> But 99% of the logic should remain the same :)</p>
<h3>Part 1: Structs</h3>
<p>We define 3 structs for storing model config, model weights &amp; to store intermediate values (run state) during forward pass</p>
<ol>
<li><strong>Config struct</strong>: Defines the transformer model.<!-- -->
<ol>
<li><code class="">n_layers</code> , <code class="">vocab_size</code>  : no. of layers (e.g. llama-2 has 32 layers/BERT-base has 12 layers) &amp; no. of tokens in our vocabulary (this is usually 30k for english languages)</li>
<li><code class="">dim</code> and <code class="">hidden_dim</code> : Define shape of Q, K, V &amp; O <code class="">(dim,dim)</code> and W1, W2 <code class="">(dim, hidden_dim)</code>&amp; W3 <code class="">(hidden_dim, dim)</code></li>
<li><code class="">n_heads</code> : Number of heads for query(Q). If <code class="">n_heads=12</code> then matrix <code class="">Q=(768,768)</code> behaves/viewed as <code class="">(768, 768/12,768)</code></li>
<li><code class="">n_kv_heads</code> : Number of heads for K &amp; V. <strong>Why are these different from above?</strong> : Read <a href="https://arxiv.org/pdf/1911.02150.pdf">multi query paper</a></li>
<li><code class="">seq_len</code> : No. of tokens we will generate</li>
</ol>
</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>typedef struct </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> dim</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> transformer dimension
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> hidden_dim</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#c792ea">for</span><span> ffn layers
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> n_layers</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> number of layers
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> n_heads</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> number of query heads
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> n_kv_heads</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> number of key</span><span class="token token" style="color:#89ddff">/</span><span>value heads </span><span class="token token" style="color:#89ddff">(</span><span>can be </span><span class="token token" style="color:#89ddff">&lt;</span><span> query heads because of multiquery</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> vocab_size</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> vocabulary size</span><span class="token token" style="color:#89ddff">,</span><span> usually </span><span class="token token" style="color:#fd9170">256</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>byte</span><span class="token token" style="color:#89ddff">-</span><span>level</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> seq_len</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#ffcb6b">max</span><span> sequence length
</span><span></span><span class="token token" style="color:#89ddff">}</span><span> Config</span><span class="token token" style="color:#89ddff">;</span></code></div></pre>
<hr/>
<ol start="2">
<li><strong>Weight struct</strong> for llama. This is our pytorch <code class="">ffn=nn.Linear(...)</code> counterpart.<!-- -->
<ol>
<li>Why are they <code class="">float*</code>  ? Because all matrices are just 1d flattened array. See below diagram</li>
<li>code is self explanatory with shapes commented.   <code class="">rms_</code>  are weights used for normalization &amp; <code class="">freq_cis_</code> are for <a href="https://arxiv.org/pdf/2104.09864.pdf">RoPE embedding</a>. We will look at <code class="">RoPE</code> in detail ahead.</li>
<li><code class="">wcls</code> is the final classifier. Matrix of size <code class="">(vocab, dim)</code> that maps final embedding from a vector to probability for each token in vocab.</li>
</ol>
</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>typedef struct </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> token embedding table
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> token_embedding_table</span><span class="token token" style="color:#89ddff">;</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>vocab_size</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> weights </span><span class="token token" style="color:#c792ea">for</span><span> rmsnorms
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> rms_att_weight</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>layer</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span> rmsnorm weights
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> rms_ffn_weight</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>layer</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> weights </span><span class="token token" style="color:#c792ea">for</span><span> matmuls
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> wq</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>layer</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> wk</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>layer</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> wv</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>layer</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> wo</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>layer</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> weights </span><span class="token token" style="color:#c792ea">for</span><span> ffn
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> w1</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>layer</span><span class="token token" style="color:#89ddff">,</span><span> hidden_dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> w2</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>layer</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">,</span><span> hidden_dim</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> w3</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>layer</span><span class="token token" style="color:#89ddff">,</span><span> hidden_dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> final rmsnorm
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> rms_final_weight</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> freq_cis </span><span class="token token" style="color:#c792ea">for</span><span> RoPE relatively positional embeddings
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> freq_cis_real</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>seq_len</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">/</span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> freq_cis_imag</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>seq_len</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">/</span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>optional</span><span class="token token" style="color:#89ddff">)</span><span> classifier weights </span><span class="token token" style="color:#c792ea">for</span><span> the logits</span><span class="token token" style="color:#89ddff">,</span><span> on the last layer
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> wcls</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span></span><span class="token token" style="color:#89ddff">}</span><span> TransformerWeights</span><span class="token token" style="color:#89ddff">;</span></code></div></pre>
<p><img src="https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/arr.png" alt="Array"/></p>
<hr/>
<ol start="3">
<li>Intermediate activations (Run state)<!-- -->
<ol>
<li>During forward pass we need to store intermediate values, e.g. output of matmul or output after norm. Will take a look at all variables later</li>
<li><code class="">key_cahce</code> and <code class="">value_cache</code> store the key, value outputs of previous tokens. e.g. during inference if the 5th token is being generated, this will store <code class="">key</code>, <code class="">value</code> of the previous 4.</li>
</ol>
</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>typedef struct </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> current wave of activations
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span> </span><span class="token token" style="color:#89ddff">*</span><span>x</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> activation at current time stamp </span><span class="token token" style="color:#89ddff">(</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span> </span><span class="token token" style="color:#89ddff">*</span><span>xb</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> same</span><span class="token token" style="color:#89ddff">,</span><span> but inside a residual branch </span><span class="token token" style="color:#89ddff">(</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span> </span><span class="token token" style="color:#89ddff">*</span><span>xb2</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> an additional </span><span class="token token" style="color:#ffcb6b">buffer</span><span> just </span><span class="token token" style="color:#c792ea">for</span><span> convenience </span><span class="token token" style="color:#89ddff">(</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span> </span><span class="token token" style="color:#89ddff">*</span><span>hb</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#ffcb6b">buffer</span><span> </span><span class="token token" style="color:#c792ea">for</span><span> hidden dimension </span><span class="token token" style="color:#c792ea">in</span><span> the ffn </span><span class="token token" style="color:#89ddff">(</span><span>hidden_dim</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span> </span><span class="token token" style="color:#89ddff">*</span><span>hb2</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#ffcb6b">buffer</span><span> </span><span class="token token" style="color:#c792ea">for</span><span> hidden dimension </span><span class="token token" style="color:#c792ea">in</span><span> the ffn </span><span class="token token" style="color:#89ddff">(</span><span>hidden_dim</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span> </span><span class="token token" style="color:#89ddff">*</span><span>q</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> query </span><span class="token token" style="color:#89ddff">(</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span> </span><span class="token token" style="color:#89ddff">*</span><span>k</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> key </span><span class="token token" style="color:#89ddff">(</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span> </span><span class="token token" style="color:#89ddff">*</span><span>v</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> value </span><span class="token token" style="color:#89ddff">(</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span> </span><span class="token token" style="color:#89ddff">*</span><span>att</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#ffcb6b">buffer</span><span> </span><span class="token token" style="color:#c792ea">for</span><span> scores</span><span class="token token" style="color:#89ddff">/</span><span>attention values </span><span class="token token" style="color:#89ddff">(</span><span>n_heads</span><span class="token token" style="color:#89ddff">,</span><span> seq_len</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span> </span><span class="token token" style="color:#89ddff">*</span><span>logits</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> output logits
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> kv cache
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> key_cache</span><span class="token token" style="color:#89ddff">;</span><span>   </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>layer</span><span class="token token" style="color:#89ddff">,</span><span> seq_len</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> value_cache</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>layer</span><span class="token token" style="color:#89ddff">,</span><span> seq_len</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span></span><span class="token token" style="color:#89ddff">}</span><span> RunState</span><span class="token token" style="color:#89ddff">;</span></code></div></pre>
<hr/>
<p>We will take a look at functions as we encounter them. For now lets see the logic inside <code class="">main()</code></p>
<h3>Part 2: Main (Can skip this part if you are only interested in <a href="#actual-forward-pass">forward logic</a> )</h3>
<ol>
<li>Get command line arguments. Nothing interesting.  Currently you can call <code class="">run.c</code> with<!-- -->
<ol>
<li><code class="">./run llama2_7b.bin</code></li>
<li><code class="">./run llama2_7b.bin 0.1</code> -&gt; with temperature</li>
<li><code class="">./run llama2_7b.bin 0.1 100</code> -&gt; with temperature &amp; steps (no. of output tokens generated)</li>
</ol>
</li>
<li>Declare <code class="">config</code> &amp; <code class="">weights</code> in the end</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#ffcb6b">int</span><span> main</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> argc</span><span class="token token" style="color:#89ddff">,</span><span> char </span><span class="token token" style="color:#89ddff">*</span><span>argv</span><span class="token token" style="color:#89ddff">[</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> poor man&#x27;s C argparse
</span><span>    char </span><span class="token token" style="color:#89ddff">*</span><span>checkpoint </span><span class="token token" style="color:#89ddff">=</span><span> NULL</span><span class="token token" style="color:#89ddff">;</span><span>  </span><span class="token token" style="color:#89ddff">//</span><span> e</span><span class="token token" style="color:#89ddff">.</span><span>g</span><span class="token token" style="color:#89ddff">.</span><span> out</span><span class="token token" style="color:#89ddff">/</span><span>model</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#ffcb6b">bin</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span> temperature </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">.</span><span>9f</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> e</span><span class="token token" style="color:#89ddff">.</span><span>g</span><span class="token token" style="color:#89ddff">.</span><span> </span><span class="token token" style="color:#fd9170">1.0</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#c792ea">or</span><span> </span><span class="token token" style="color:#fd9170">0.0</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> steps </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">256</span><span class="token token" style="color:#89ddff">;</span><span>          </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#ffcb6b">max</span><span> number of steps to run </span><span class="token token" style="color:#c792ea">for</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">:</span><span> use seq_len
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#a5e844">&#x27;checkpoint&#x27;</span><span> </span><span class="token token" style="color:#c792ea">is</span><span> necessary arg
</span><span>    </span><span class="token token" style="color:#c792ea">if</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>argc </span><span class="token token" style="color:#89ddff">&lt;</span><span> </span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>        printf</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#a5e844">&quot;Usage: %s &lt;checkpoint_file&gt; [temperature] [steps]\n&quot;</span><span class="token token" style="color:#89ddff">,</span><span> argv</span><span class="token token" style="color:#89ddff">[</span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        </span><span class="token token" style="color:#c792ea">return</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>    </span><span class="token token" style="color:#c792ea">if</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>argc </span><span class="token token" style="color:#89ddff">&gt;=</span><span> </span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>        checkpoint </span><span class="token token" style="color:#89ddff">=</span><span> argv</span><span class="token token" style="color:#89ddff">[</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>    </span><span class="token token" style="color:#c792ea">if</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>argc </span><span class="token token" style="color:#89ddff">&gt;=</span><span> </span><span class="token token" style="color:#fd9170">3</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>        </span><span class="token token" style="color:#89ddff">//</span><span> optional temperature</span><span class="token token" style="color:#89ddff">.</span><span> </span><span class="token token" style="color:#fd9170">0.0</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>deterministic</span><span class="token token" style="color:#89ddff">)</span><span> argmax sampling</span><span class="token token" style="color:#89ddff">.</span><span> </span><span class="token token" style="color:#fd9170">1.0</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> baseline
</span><span>        temperature </span><span class="token token" style="color:#89ddff">=</span><span> atof</span><span class="token token" style="color:#89ddff">(</span><span>argv</span><span class="token token" style="color:#89ddff">[</span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>    </span><span class="token token" style="color:#c792ea">if</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>argc </span><span class="token token" style="color:#89ddff">&gt;=</span><span> </span><span class="token token" style="color:#fd9170">4</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>        steps </span><span class="token token" style="color:#89ddff">=</span><span> atoi</span><span class="token token" style="color:#89ddff">(</span><span>argv</span><span class="token token" style="color:#89ddff">[</span><span class="token token" style="color:#fd9170">3</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>	</span><span class="token token" style="color:#89ddff">//</span><span> seed rng </span><span class="token token" style="color:#c792ea">with</span><span> time</span><span class="token token" style="color:#89ddff">.</span><span> </span><span class="token token" style="color:#c792ea">if</span><span> you want deterministic behavior use temperature </span><span class="token token" style="color:#fd9170">0.0</span><span>
</span><span>    srand</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">(</span><span>unsigned </span><span class="token token" style="color:#ffcb6b">int</span><span class="token token" style="color:#89ddff">)</span><span>time</span><span class="token token" style="color:#89ddff">(</span><span>NULL</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span> 
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> read </span><span class="token token" style="color:#c792ea">in</span><span> the model</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#ffcb6b">bin</span><span> </span><span class="token token" style="color:#ffcb6b">file</span><span>
</span><span>    Config config</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    TransformerWeights weights</span><span class="token token" style="color:#89ddff">;</span></code></div></pre>
<ol start="2">
<li>
<p>Reading <code class="">checkpoint</code> file.</p>
<ol>
<li>If you are familiar with PyTorch. Usually  <code class="">config.json</code> &amp; <code class="">model.bin</code> are separate (we load weights like a dictionary). But here <code class="">train.py</code> saves everything in one <code class="">.bin</code>  file in a specific format. This specific format allows us to easily read config &amp; then each weight one by one.</li>
</ol>
<p>Details</p>
<ol>
<li><code class="">shared_weights</code> : Should input embedding matrix &amp; output classifier matrix be same?</li>
<li>Next load into <code class="">weights</code>. Get file size via <code class="">file_size = ftell(file);</code> Unlike vanilla PyTorch inference we <strong>don&#x27;t</strong> load all weights into RAM. Instead we call <code class="">mmap(..)</code> to allocate RAM memory when we want lazily. For more detail <a href="https://stackoverflow.com/questions/5877797/how-does-mmap-work">read</a></li>
<li>Finally  call <code class="">checkpoint_init_weights</code>  (snippet of function below). Here we map our weight pointers to correct address returned by <code class="">mmap</code>. Since we already read config we offset for it in line  <code class="">float* weights_ptr = data + sizeof(Config)/sizeof(float);</code></li>
</ol>
</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>void checkpoint_init_weights</span><span class="token token" style="color:#89ddff">(</span><span>TransformerWeights </span><span class="token token" style="color:#89ddff">*</span><span>w</span><span class="token token" style="color:#89ddff">,</span><span> Config</span><span class="token token" style="color:#89ddff">*</span><span> p</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> f</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#ffcb6b">int</span><span> shared_weights</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">{</span><span>
</span><span></span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> ptr </span><span class="token token" style="color:#89ddff">=</span><span> f</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>token_embedding_table </span><span class="token token" style="color:#89ddff">=</span><span> ptr</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>ptr </span><span class="token token" style="color:#89ddff">+=</span><span> p</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>vocab_size </span><span class="token token" style="color:#89ddff">*</span><span> p</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>dim</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>rms_att_weight </span><span class="token token" style="color:#89ddff">=</span><span> ptr</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span></span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#89ddff">.</span><span>
</span><span></span><span class="token token" style="color:#89ddff">}</span></code></div></pre>
<p>Original code we are talking about in above section</p>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> fd </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> data </span><span class="token token" style="color:#89ddff">=</span><span> NULL</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">long</span><span> file_size</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>        FILE </span><span class="token token" style="color:#89ddff">*</span><span class="token token" style="color:#ffcb6b">file</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> fopen</span><span class="token token" style="color:#89ddff">(</span><span>checkpoint</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#a5e844">&quot;rb&quot;</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        </span><span class="token token" style="color:#c792ea">if</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>!</span><span class="token token" style="color:#ffcb6b">file</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>            printf</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#a5e844">&quot;Unable to open the checkpoint file %s!\n&quot;</span><span class="token token" style="color:#89ddff">,</span><span> checkpoint</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>            </span><span class="token token" style="color:#c792ea">return</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        </span><span class="token token" style="color:#89ddff">}</span><span> 
</span><span>	    </span><span class="token token" style="color:#89ddff">//</span><span> read </span><span class="token token" style="color:#c792ea">in</span><span> the config header
</span><span>        </span><span class="token token" style="color:#c792ea">if</span><span class="token token" style="color:#89ddff">(</span><span>fread</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">&amp;</span><span>config</span><span class="token token" style="color:#89ddff">,</span><span> sizeof</span><span class="token token" style="color:#89ddff">(</span><span>Config</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#ffcb6b">file</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">!=</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span> </span><span class="token token" style="color:#c792ea">return</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>        </span><span class="token token" style="color:#89ddff">//</span><span> negative vocab size </span><span class="token token" style="color:#c792ea">is</span><span> hacky way of signaling unshared weights</span><span class="token token" style="color:#89ddff">.</span><span> bit yikes</span><span class="token token" style="color:#89ddff">.</span><span>
</span><span>        </span><span class="token token" style="color:#ffcb6b">int</span><span> shared_weights </span><span class="token token" style="color:#89ddff">=</span><span> config</span><span class="token token" style="color:#89ddff">.</span><span>vocab_size </span><span class="token token" style="color:#89ddff">&gt;</span><span> </span><span class="token token" style="color:#fd9170">0</span><span> ? </span><span class="token token" style="color:#fd9170">1</span><span> </span><span class="token token" style="color:#89ddff">:</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        config</span><span class="token token" style="color:#89ddff">.</span><span>vocab_size </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#ffcb6b">abs</span><span class="token token" style="color:#89ddff">(</span><span>config</span><span class="token token" style="color:#89ddff">.</span><span>vocab_size</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        </span><span class="token token" style="color:#89ddff">//</span><span> figure out the </span><span class="token token" style="color:#ffcb6b">file</span><span> size
</span><span>        fseek</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">file</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">,</span><span> SEEK_END</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> move </span><span class="token token" style="color:#ffcb6b">file</span><span> pointer to end of </span><span class="token token" style="color:#ffcb6b">file</span><span>
</span><span>        file_size </span><span class="token token" style="color:#89ddff">=</span><span> ftell</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">file</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> get the </span><span class="token token" style="color:#ffcb6b">file</span><span> size</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#c792ea">in</span><span> </span><span class="token token" style="color:#ffcb6b">bytes</span><span>
</span><span>        fclose</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">file</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span>        
<span>        </span><span class="token token" style="color:#89ddff">//</span><span> memory </span><span class="token token" style="color:#ffcb6b">map</span><span> the Transformer weights into the data pointer
</span><span>        fd </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#ffcb6b">open</span><span class="token token" style="color:#89ddff">(</span><span>checkpoint</span><span class="token token" style="color:#89ddff">,</span><span> O_RDONLY</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#ffcb6b">open</span><span> </span><span class="token token" style="color:#c792ea">in</span><span> read only mode
</span><span>        </span><span class="token token" style="color:#c792ea">if</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>fd </span><span class="token token" style="color:#89ddff">==</span><span> </span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span> printf</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#a5e844">&quot;open failed!\n&quot;</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#c792ea">return</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>        data </span><span class="token token" style="color:#89ddff">=</span><span> mmap</span><span class="token token" style="color:#89ddff">(</span><span>NULL</span><span class="token token" style="color:#89ddff">,</span><span> file_size</span><span class="token token" style="color:#89ddff">,</span><span> PROT_READ</span><span class="token token" style="color:#89ddff">,</span><span> MAP_PRIVATE</span><span class="token token" style="color:#89ddff">,</span><span> fd</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        </span><span class="token token" style="color:#c792ea">if</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>data </span><span class="token token" style="color:#89ddff">==</span><span> MAP_FAILED</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span> printf</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#a5e844">&quot;mmap failed!\n&quot;</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#c792ea">return</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>        </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> weights_ptr </span><span class="token token" style="color:#89ddff">=</span><span> data </span><span class="token token" style="color:#89ddff">+</span><span> sizeof</span><span class="token token" style="color:#89ddff">(</span><span>Config</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">/</span><span>sizeof</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        checkpoint_init_weights</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">&amp;</span><span>weights</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#89ddff">&amp;</span><span>config</span><span class="token token" style="color:#89ddff">,</span><span> weights_ptr</span><span class="token token" style="color:#89ddff">,</span><span> shared_weights</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">}</span></code></div></pre>
<hr/>
<ol start="3">
<li>Reading vocab file -&gt; Mostly straightforward, only few details<!-- -->
<ol>
<li><code class="">vocab</code> is <code class="">char**</code> since each token is a string &amp; <code class="">vocab</code> is a list of tokens.</li>
<li>For loop over <code class="">vocab_size</code> &amp; read each token</li>
</ol>
</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#89ddff">//</span><span> right now we cannot run </span><span class="token token" style="color:#c792ea">for</span><span> more than config</span><span class="token token" style="color:#89ddff">.</span><span>seq_len steps
</span><span>    </span><span class="token token" style="color:#c792ea">if</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>steps </span><span class="token token" style="color:#89ddff">&lt;=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span> </span><span class="token token" style="color:#89ddff">|</span><span class="token token" style="color:#89ddff">|</span><span> steps </span><span class="token token" style="color:#89ddff">&gt;</span><span> config</span><span class="token token" style="color:#89ddff">.</span><span>seq_len</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span> steps </span><span class="token token" style="color:#89ddff">=</span><span> config</span><span class="token token" style="color:#89ddff">.</span><span>seq_len</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> read </span><span class="token token" style="color:#c792ea">in</span><span> the tokenizer</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#ffcb6b">bin</span><span> </span><span class="token token" style="color:#ffcb6b">file</span><span>
</span><span>    char</span><span class="token token" style="color:#89ddff">**</span><span> vocab </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>char</span><span class="token token" style="color:#89ddff">**</span><span class="token token" style="color:#89ddff">)</span><span>malloc</span><span class="token token" style="color:#89ddff">(</span><span>config</span><span class="token token" style="color:#89ddff">.</span><span>vocab_size </span><span class="token token" style="color:#89ddff">*</span><span> sizeof</span><span class="token token" style="color:#89ddff">(</span><span>char</span><span class="token token" style="color:#89ddff">*</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>        FILE </span><span class="token token" style="color:#89ddff">*</span><span class="token token" style="color:#ffcb6b">file</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> fopen</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#a5e844">&quot;tokenizer.bin&quot;</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#a5e844">&quot;rb&quot;</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        </span><span class="token token" style="color:#c792ea">if</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>!</span><span class="token token" style="color:#ffcb6b">file</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>            printf</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#a5e844">&quot;Unable to open the tokenizer file tokenizer.bin! Run &quot;</span><span>
</span><span>            </span><span class="token token" style="color:#a5e844">&quot;python tokenizer.py to convert tokenizer.model -&gt; tokenizer.bin\n&quot;</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>            </span><span class="token token" style="color:#c792ea">return</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>        </span><span class="token token" style="color:#ffcb6b">int</span><span> </span><span class="token token" style="color:#ffcb6b">len</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        </span><span class="token token" style="color:#c792ea">for</span><span> </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> i </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span> i </span><span class="token token" style="color:#89ddff">&lt;</span><span> config</span><span class="token token" style="color:#89ddff">.</span><span>vocab_size</span><span class="token token" style="color:#89ddff">;</span><span> i</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>            </span><span class="token token" style="color:#c792ea">if</span><span class="token token" style="color:#89ddff">(</span><span>fread</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">&amp;</span><span class="token token" style="color:#ffcb6b">len</span><span class="token token" style="color:#89ddff">,</span><span> sizeof</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#ffcb6b">file</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">!=</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span> </span><span class="token token" style="color:#c792ea">return</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>            vocab</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>char </span><span class="token token" style="color:#89ddff">*</span><span class="token token" style="color:#89ddff">)</span><span>malloc</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">len</span><span> </span><span class="token token" style="color:#89ddff">+</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>            </span><span class="token token" style="color:#c792ea">if</span><span class="token token" style="color:#89ddff">(</span><span>fread</span><span class="token token" style="color:#89ddff">(</span><span>vocab</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#ffcb6b">len</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#ffcb6b">file</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">!=</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span> </span><span class="token token" style="color:#c792ea">return</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>            vocab</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">[</span><span class="token token" style="color:#ffcb6b">len</span><span class="token token" style="color:#89ddff">]</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#a5e844">&#x27;\0&#x27;</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> add the string terminating token
</span><span>        </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>        fclose</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">file</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">}</span></code></div></pre>
<hr/>
<h4>Forward Loop &amp; sampling in main (Go to <a href="#actual-forward-pass">important part</a>)</h4>
<ol>
<li>Allocate memory for run state/intermediate values. The first <code class="">token</code> we pass into our model is BOS token (&quot;Beginning of Statement&quot;) who&#x27;s vocab index is <code class="">1</code>.</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>    RunState state</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    malloc_run_state</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">&amp;</span><span>state</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#89ddff">&amp;</span><span>config</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span>    
<span>    </span><span class="token token" style="color:#89ddff">//</span><span> the current position we are </span><span class="token token" style="color:#c792ea">in</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">long</span><span> start </span><span class="token token" style="color:#89ddff">=</span><span> time_in_ms</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> </span><span class="token token" style="color:#ffcb6b">next</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> token </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#fd9170">1</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> BOS token </span><span class="token token" style="color:#c792ea">in</span><span> Llama</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#fd9170">2</span><span> sentencepiece
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> pos </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    printf</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#a5e844">&quot;&lt;s&gt;\n&quot;</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> explicit </span><span class="token token" style="color:#c792ea">print</span><span> the initial BOS token </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">=</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">,</span><span> stylistically symmetric</span></code></div></pre>
<p><img src="https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs//luke.jpeg" alt="Luke"/></p>
<ol start="2">
<li>Forward loop:<!-- -->
<ol>
<li>
<p><code class="">transformer(token, pos, &amp;config, &amp;state, &amp;weights);</code> stores classifier score of each token as being the next token in sequence inside <code class="">state.logits</code>.(contents of <code class="">transformer</code> function convered in next section).</p>
</li>
<li>
<p>Next we sample. <strong>Why we need sampling &amp; how to do it?</strong></p>
<ul>
<li>Lets say you want AI to complete dialogues of a movie &amp; your input is <em>&quot;Luke, I am your&quot;</em> . Now <code class="">llama</code> gives you score for each token to be the next word. So e.g. assume our tokens are <code class="">[&quot;Apple&quot;, &quot;Football&quot;, &quot;Father&quot;, &quot;Brother&quot;]</code> &amp; llama gives them scores of <code class="">[0.3, 0.1, 0.9, 0.7]</code>. Now to pick the next token, either we take maximum (<code class="">&quot;Father&quot;</code> with score 0.9) or we sample tokens with a probability proportional to thier score, this way we can get more diversity(very important in today&#x27;s world ) in our prediction.</li>
</ul>
</li>
<li>
<p>Lets discuss some more details: If <code class="">temperature=0</code> then its max sampling.  For <code class="">temperate&gt;0</code> we convert <code class="">state.logits</code> into probabilities using softmax &amp; store back in <code class="">state.logits</code>. The <code class="">sample(..)</code> function returns a token sampled from the <code class="">state.logits</code> probability distribution. Read more <a href="https://web.mit.edu/urban_or_book/www/book/chapter7/7.1.3.html">here</a></p>
</li>
<li>
<p>The token generated <code class="">next</code> becomes the next input token in line <code class="">token=next</code>.</p>
</li>
</ol>
</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#c792ea">while</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>pos </span><span class="token token" style="color:#89ddff">&lt;</span><span> steps</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>        </span><span class="token token" style="color:#89ddff">//</span><span> forward the transformer to get logits </span><span class="token token" style="color:#c792ea">for</span><span> the </span><span class="token token" style="color:#ffcb6b">next</span><span> token
</span><span>        transformer</span><span class="token token" style="color:#89ddff">(</span><span>token</span><span class="token token" style="color:#89ddff">,</span><span> pos</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#89ddff">&amp;</span><span>config</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#89ddff">&amp;</span><span>state</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#89ddff">&amp;</span><span>weights</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        </span><span class="token token" style="color:#89ddff">//</span><span> sample the </span><span class="token token" style="color:#ffcb6b">next</span><span> token
</span><span>        </span><span class="token token" style="color:#c792ea">if</span><span class="token token" style="color:#89ddff">(</span><span>temperature </span><span class="token token" style="color:#89ddff">==</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">.</span><span>0f</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>            </span><span class="token token" style="color:#89ddff">//</span><span> greedy argmax sampling
</span><span>            </span><span class="token token" style="color:#ffcb6b">next</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> argmax</span><span class="token token" style="color:#89ddff">(</span><span>state</span><span class="token token" style="color:#89ddff">.</span><span>logits</span><span class="token token" style="color:#89ddff">,</span><span> config</span><span class="token token" style="color:#89ddff">.</span><span>vocab_size</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        </span><span class="token token" style="color:#89ddff">}</span><span> </span><span class="token token" style="color:#c792ea">else</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>            </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#ffcb6b">apply</span><span> the temperature to the logits
</span><span>            </span><span class="token token" style="color:#c792ea">for</span><span> </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> q</span><span class="token token" style="color:#89ddff">=</span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span> q</span><span class="token token" style="color:#89ddff">&lt;</span><span>config</span><span class="token token" style="color:#89ddff">.</span><span>vocab_size</span><span class="token token" style="color:#89ddff">;</span><span> q</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span> state</span><span class="token token" style="color:#89ddff">.</span><span>logits</span><span class="token token" style="color:#89ddff">[</span><span>q</span><span class="token token" style="color:#89ddff">]</span><span> </span><span class="token token" style="color:#89ddff">/=</span><span> temperature</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>            </span><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#ffcb6b">apply</span><span> softmax to the logits to get the probabilities </span><span class="token token" style="color:#c792ea">for</span><span> </span><span class="token token" style="color:#ffcb6b">next</span><span> token
</span><span>            softmax</span><span class="token token" style="color:#89ddff">(</span><span>state</span><span class="token token" style="color:#89ddff">.</span><span>logits</span><span class="token token" style="color:#89ddff">,</span><span> config</span><span class="token token" style="color:#89ddff">.</span><span>vocab_size</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>            </span><span class="token token" style="color:#89ddff">//</span><span> we now want to sample </span><span class="token token" style="color:#c792ea">from</span><span> this distribution to get the </span><span class="token token" style="color:#ffcb6b">next</span><span> token
</span><span>            </span><span class="token token" style="color:#ffcb6b">next</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> sample</span><span class="token token" style="color:#89ddff">(</span><span>state</span><span class="token token" style="color:#89ddff">.</span><span>logits</span><span class="token token" style="color:#89ddff">,</span><span> config</span><span class="token token" style="color:#89ddff">.</span><span>vocab_size</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>        printf</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#a5e844">&quot;%s&quot;</span><span class="token token" style="color:#89ddff">,</span><span> vocab</span><span class="token token" style="color:#89ddff">[</span><span class="token token" style="color:#ffcb6b">next</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        fflush</span><span class="token token" style="color:#89ddff">(</span><span>stdout</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span>
<span>        </span><span class="token token" style="color:#89ddff">//</span><span> advance forward
</span><span>        token </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#ffcb6b">next</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>        pos</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">}</span></code></div></pre>
<hr/>
<h3>Actual Forward pass</h3>
<p>Details of <code class="">transformer(token, pos, &amp;config, &amp;state, &amp;weights);</code> called from <code class="">main()</code></p>
<p>Section below uses 2d/3d array indexing extensively. We cover it briefly here to make life easier</p>
<ol>
<li>If matrix <code class="">float* mat</code> is of size <code class="">(dim1, dim2, dim3)</code> then pointer to access <code class="">mat[l][i][j]</code> is <code class="">dim2*dim3*l + dim3*i + j;</code> -  This is <code class="">formula-1</code> we will refer to this often later. Read <a href="https://www.learncpp.com/cpp-tutorial/pointer-arithmetic-and-array-indexing/">link</a> if you are confused</li>
</ol>
<p>How to view matrices in terms of head?</p>
<ol>
<li>K (key) <code class="">float* wk</code> is a matrix defined as shape <code class="">(layer, dim, dim)</code> when viewed in terms of heads is <code class="">(layer, dim, n_heads, head_dim)</code></li>
</ol>
<hr/>
<ol>
<li>Convenience variables. Nothing interesting apart from copying the embedding of <code class="">token</code> into <code class="">s-&gt;xb</code> using <code class="">memcpy</code>. Why not use <code class="">float* content_row</code> itself? Because  <code class="">s-&gt;xb</code> is going to change &amp; using <code class="">content_row</code> will change model weights.</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>void transformer</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> token</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#ffcb6b">int</span><span> pos</span><span class="token token" style="color:#89ddff">,</span><span> Config</span><span class="token token" style="color:#89ddff">*</span><span> p</span><span class="token token" style="color:#89ddff">,</span><span> RunState</span><span class="token token" style="color:#89ddff">*</span><span> s</span><span class="token token" style="color:#89ddff">,</span><span> TransformerWeights</span><span class="token token" style="color:#89ddff">*</span><span> w</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> a few convenience variables
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span> </span><span class="token token" style="color:#89ddff">*</span><span>x </span><span class="token token" style="color:#89ddff">=</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>x</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> dim </span><span class="token token" style="color:#89ddff">=</span><span> p</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>dim</span><span class="token token" style="color:#89ddff">;</span><span>                  
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> hidden_dim </span><span class="token token" style="color:#89ddff">=</span><span>  p</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>hidden_dim</span><span class="token token" style="color:#89ddff">;</span><span>  
</span><span>    </span><span class="token token" style="color:#ffcb6b">int</span><span> head_size </span><span class="token token" style="color:#89ddff">=</span><span> dim </span><span class="token token" style="color:#89ddff">/</span><span> p</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>n_heads</span><span class="token token" style="color:#89ddff">;</span><span> 
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> content_row </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#89ddff">&amp;</span><span class="token token" style="color:#89ddff">(</span><span>w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>token_embedding_table</span><span class="token token" style="color:#89ddff">[</span><span>token </span><span class="token token" style="color:#89ddff">*</span><span> dim</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#89ddff">//</span><span> copy the token embedding into x
</span><span>    memcpy</span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">,</span><span> content_row</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">*</span><span>sizeof</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">*</span><span>x</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span> </span></code></div></pre>
<hr/>
<p><strong>RoPE</strong> : Rotary Positional Embeddings</p>
<ul>
<li>Formulation:  Transforms feature pairs by rotating it in 2D plane.
e.g. If your vector is <code class="">[0.8, 0.5, -0.1, 0.3]</code> we group them into pairs: <code class="">[[0.8,-0.1], [0.5, 0.3]</code> and rotate by some angle $\theta$. This $\theta$ is ~~part of the weights &amp; is learned during training~~ $\theta$ is fixed from the start (its not learnable). In the paper the value of $\theta_{i}$ is $10000^{2(i-1)/d}$</li>
</ul>
<p>RoPE  Formula (For 2 features grouped into a pair) is below. $m$ is the index of the pair. $\theta$ is a learned parameter that we load from <code class="">.bin</code> file</p>
<p>$$
\left[ {\begin{array}{ccccc}
x_{m}^{i} &amp; x_{m}^{j} \
\end{array} } \right] * \left[ {\begin{array}{ccccc}
cos(m\theta_{m}) &amp; -sin(m\theta_{m}) \
sin(m\theta_{m}) &amp; cos(m\theta_{m}) \
\end{array} } \right]
$$</p>
<p>Our example pair <code class="">[[0.8,-0.1], [0.5, 0.3]</code>  will be transformed like below. Keep in mind for the first pair <code class="">[0.8, 0.1]</code> $m=0$ since (therefore $sin(0)=0$). And for 2nd pair <code class="">m=1</code></p>
<p>$$
\left[ {\begin{array}{ccccc}
0.8 &amp; -0.1 \
\end{array} } \right] * \left[ {\begin{array}{ccccc}
1 * 1 &amp; -0.0 * 1 \
0.0 * 1 &amp; 1.0 * 1 \
\end{array} } \right] =    \left[ {\begin{array}{ccccc}
0.8 &amp; -0.1 \
\end{array} } \right]
$$</p>
<p>$$
\left[ {\begin{array}{ccccc}
0.5 &amp; 0.3 \
\end{array} } \right] * \left[ {\begin{array}{ccccc}
0.86 * 1 &amp; -0.5 * 1 \
0.5 * 1 &amp; 0.86 * 1 \
\end{array} } \right] =    \left[ {\begin{array}{ccccc}
0.58 &amp; 0.08 \
\end{array} } \right]
$$</p>
<p>Combining both, the output is <code class="">[[0.8, 0.1], [0.58, 0.08]]</code> now <strong>un-pairing</strong> them will give us <code class="">[0.8, 0.58, 0.1, 0.08]</code>
So <code class="">RoPE</code> transformed <code class="">[0.8, 0.5, -0.1, 0.3]</code> into <code class="">[0.8, 0.58, 0.1, 0.08]</code>. Keep in mind if a feature is of <code class="">dim=768</code> then there are half of it <strong>384</strong> learnable $\theta$&#x27;s.</p>
<p><strong>Back to code</strong></p>
<ol>
<li>We get $\theta$ for current position (<code class="">pos</code> is our $m$).  <code class="">freq_cis_real_row</code> is $cos(m\theta)$ and <code class="">freq_cis_imag_row</code> is $sin(m\theta)$.</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>    </span><span class="token token" style="color:#89ddff">//</span><span> pluck out the </span><span class="token token" style="color:#a5e844">&quot;pos&quot;</span><span> row of freq_cis_real </span><span class="token token" style="color:#c792ea">and</span><span> freq_cis_imag66
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> freq_cis_real_row </span><span class="token token" style="color:#89ddff">=</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>freq_cis_real </span><span class="token token" style="color:#89ddff">+</span><span> pos </span><span class="token token" style="color:#89ddff">*</span><span> head_size </span><span class="token token" style="color:#89ddff">/</span><span> </span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>    </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> freq_cis_imag_row </span><span class="token token" style="color:#89ddff">=</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>freq_cis_imag </span><span class="token token" style="color:#89ddff">+</span><span> pos </span><span class="token token" style="color:#89ddff">*</span><span> head_size </span><span class="token token" style="color:#89ddff">/</span><span> </span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">;</span></code></div></pre>
<ol start="2">
<li>Iterate over layers. Apply <code class="">rmsnorm</code> to input of the layer.  <code class="">rmsnorm</code> function calculates the below</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>out\</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> \</span><span class="token token" style="color:#89ddff">;</span><span>  </span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">*</span><span>g</span><span class="token token" style="color:#89ddff">*</span><span>n</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">/</span><span>\sum_</span><span class="token token" style="color:#89ddff">{</span><span>i</span><span class="token token" style="color:#89ddff">}</span><span> \sqrt</span><span class="token token" style="color:#89ddff">{</span><span>x_</span><span class="token token" style="color:#89ddff">{</span><span>i</span><span class="token token" style="color:#89ddff">}</span><span class="token token" style="color:#89ddff">^</span><span class="token token" style="color:#89ddff">{</span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">}</span><span class="token token" style="color:#89ddff">}</span><span> </span></code></div></pre>
<p>where $x$ is input, $g$ is learnable parameter (<code class="">w-&gt;rms_attn_weight</code> below) &amp; $n$ is <code class="">dim</code>.</p>
<p><code class="">matmul</code> does matrix mult of a 2d matrix with a 1d matrix. <code class="">(A, B) x (A,)</code>. The implementation is trivial (we cover this at very end). We multiply Q,K,V with <code class="">s-&gt;xb</code> (output of <code class="">rmsnorm</code>) and store output in <code class="">s-&gt;q</code>, <code class="">s-&gt;k</code> ..</p>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#c792ea">for</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> l </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span> l </span><span class="token token" style="color:#89ddff">&lt;</span><span> p</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>n_layers</span><span class="token token" style="color:#89ddff">;</span><span> l</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span></span><span class="token token" style="color:#89ddff">//</span><span> attention rmsnorm
</span><span>	rmsnorm</span><span class="token token" style="color:#89ddff">(</span><span>s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb</span><span class="token token" style="color:#89ddff">,</span><span> x</span><span class="token token" style="color:#89ddff">,</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>rms_att_weight </span><span class="token token" style="color:#89ddff">+</span><span> l</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span>	
<span>	</span><span class="token token" style="color:#89ddff">//</span><span> qkv matmuls </span><span class="token token" style="color:#c792ea">for</span><span> this position
</span><span>	matmul</span><span class="token token" style="color:#89ddff">(</span><span>s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>q</span><span class="token token" style="color:#89ddff">,</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb</span><span class="token token" style="color:#89ddff">,</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>wq </span><span class="token token" style="color:#89ddff">+</span><span> l</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>	matmul</span><span class="token token" style="color:#89ddff">(</span><span>s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>k</span><span class="token token" style="color:#89ddff">,</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb</span><span class="token token" style="color:#89ddff">,</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>wk </span><span class="token token" style="color:#89ddff">+</span><span> l</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>	matmul</span><span class="token token" style="color:#89ddff">(</span><span>s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>v</span><span class="token token" style="color:#89ddff">,</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb</span><span class="token token" style="color:#89ddff">,</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>wv </span><span class="token token" style="color:#89ddff">+</span><span> l</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span></code></div></pre>
<ol start="3">
<li>Go over each head &amp; apply the 2-d $cos$/$sin$ transformation we discussed above to <code class="">s-&gt;q</code> and <code class="">s-&gt;k</code>. We do it separately for each head, therefore we take offset of <code class="">h*head_size</code></li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#89ddff">//</span><span> </span><span class="token token" style="color:#ffcb6b">apply</span><span> RoPE rotation to the q </span><span class="token token" style="color:#c792ea">and</span><span> k vectors </span><span class="token token" style="color:#c792ea">for</span><span> each head
</span><span>        </span><span class="token token" style="color:#c792ea">for</span><span> </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> h </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span> h </span><span class="token token" style="color:#89ddff">&lt;</span><span> p</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>n_heads</span><span class="token token" style="color:#89ddff">;</span><span> h</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>            </span><span class="token token" style="color:#89ddff">//</span><span> get the q </span><span class="token token" style="color:#c792ea">and</span><span> k vectors </span><span class="token token" style="color:#c792ea">for</span><span> this head
</span><span>            </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> q </span><span class="token token" style="color:#89ddff">=</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>q </span><span class="token token" style="color:#89ddff">+</span><span> h </span><span class="token token" style="color:#89ddff">*</span><span> head_size</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>            </span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> k </span><span class="token token" style="color:#89ddff">=</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>k </span><span class="token token" style="color:#89ddff">+</span><span> h </span><span class="token token" style="color:#89ddff">*</span><span> head_size</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>            </span><span class="token token" style="color:#89ddff">//</span><span> rotate q </span><span class="token token" style="color:#c792ea">and</span><span> k by the freq_cis_real </span><span class="token token" style="color:#c792ea">and</span><span> freq_cis_imag
</span><span>            </span><span class="token token" style="color:#c792ea">for</span><span> </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> i </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span> i </span><span class="token token" style="color:#89ddff">&lt;</span><span> head_size</span><span class="token token" style="color:#89ddff">;</span><span> i</span><span class="token token" style="color:#89ddff">+=</span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>                </span><span class="token token" style="color:#ffcb6b">float</span><span> q0 </span><span class="token token" style="color:#89ddff">=</span><span> q</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>                </span><span class="token token" style="color:#ffcb6b">float</span><span> q1 </span><span class="token token" style="color:#89ddff">=</span><span> q</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>                </span><span class="token token" style="color:#ffcb6b">float</span><span> k0 </span><span class="token token" style="color:#89ddff">=</span><span> k</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>                </span><span class="token token" style="color:#ffcb6b">float</span><span> k1 </span><span class="token token" style="color:#89ddff">=</span><span> k</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>                </span><span class="token token" style="color:#ffcb6b">float</span><span> fcr </span><span class="token token" style="color:#89ddff">=</span><span> freq_cis_real_row</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">/</span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>                </span><span class="token token" style="color:#ffcb6b">float</span><span> fci </span><span class="token token" style="color:#89ddff">=</span><span> freq_cis_imag_row</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">/</span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>                q</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span>   </span><span class="token token" style="color:#89ddff">=</span><span> q0 </span><span class="token token" style="color:#89ddff">*</span><span> fcr </span><span class="token token" style="color:#89ddff">-</span><span> q1 </span><span class="token token" style="color:#89ddff">*</span><span> fci</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>                q</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">]</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> q0 </span><span class="token token" style="color:#89ddff">*</span><span> fci </span><span class="token token" style="color:#89ddff">+</span><span> q1 </span><span class="token token" style="color:#89ddff">*</span><span> fcr</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>                k</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span>   </span><span class="token token" style="color:#89ddff">=</span><span> k0 </span><span class="token token" style="color:#89ddff">*</span><span> fcr </span><span class="token token" style="color:#89ddff">-</span><span> k1 </span><span class="token token" style="color:#89ddff">*</span><span> fci</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>                k</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">]</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> k0 </span><span class="token token" style="color:#89ddff">*</span><span> fci </span><span class="token token" style="color:#89ddff">+</span><span> k1 </span><span class="token token" style="color:#89ddff">*</span><span> fcr</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>            </span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>        </span><span class="token token" style="color:#89ddff">}</span></code></div></pre>
<ol start="4">
<li>Once we get <code class="">q, k, v</code> for current token, we need to calculate self-attention. Where we multiply query into key.  <code class="">k &amp; v</code> are only for the current token. We store the <code class="">k, v</code> for all past tokens in <code class="">key_cache_row</code>  &amp; <code class="">value_cache_row</code>.<!-- -->
<ul>
<li>For example, if we have generated the tokens (&quot;fox&quot;, &quot;jumps&quot;, &quot;over&quot;) until now then we already have Q &amp; V for &quot;fox&quot; &amp; &quot;jumps&quot; from previous forward passes stored in our cache. We need not recalculate.</li>
<li>Since caches store key, query for all layers &amp; for all tokens (max no.of tokens is <code class="">seq_length</code>) its dimensions are <code class="">(layer, seq_length, dim)</code>. <code class="">seq_length</code> is usually called <code class="">context</code>.</li>
</ul>
</li>
<li>Consider below code in terms of above example. Lets say <code class="">seq_length=32</code> (which means we generate at-most 32 tokens). <code class="">pos=2</code> since &quot;fox&quot; is the 3rd token (2nd since python is 0-indexed).<!-- -->
<ul>
<li>We already have <code class="">layer*(pos-1)*dim</code> values filled in <code class="">s-&gt;key_cache</code> We need to fill the key, value of current token &quot;fox&quot; into <code class="">s-&gt;key_cache</code> too before doing self-attention. This is what <code class="">memcpy(key_cache_row, s-&gt;k, dim*sizeof(*key_cache_row));</code> does</li>
</ul>
</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#89ddff">//</span><span> save key</span><span class="token token" style="color:#89ddff">,</span><span>value at this time step </span><span class="token token" style="color:#89ddff">(</span><span>pos</span><span class="token token" style="color:#89ddff">)</span><span> to our kv cache
</span><span></span><span class="token token" style="color:#ffcb6b">int</span><span> loff </span><span class="token token" style="color:#89ddff">=</span><span> l </span><span class="token token" style="color:#89ddff">*</span><span> p</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>seq_len </span><span class="token token" style="color:#89ddff">*</span><span> dim</span><span class="token token" style="color:#89ddff">;</span><span> </span><span class="token token" style="color:#89ddff">//</span><span> kv cache layer offset </span><span class="token token" style="color:#c792ea">for</span><span> convenience
</span><span></span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> key_cache_row </span><span class="token token" style="color:#89ddff">=</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>key_cache </span><span class="token token" style="color:#89ddff">+</span><span> loff </span><span class="token token" style="color:#89ddff">+</span><span> pos </span><span class="token token" style="color:#89ddff">*</span><span> dim</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span></span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> value_cache_row </span><span class="token token" style="color:#89ddff">=</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>value_cache </span><span class="token token" style="color:#89ddff">+</span><span> loff </span><span class="token token" style="color:#89ddff">+</span><span> pos </span><span class="token token" style="color:#89ddff">*</span><span> dim</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>memcpy</span><span class="token token" style="color:#89ddff">(</span><span>key_cache_row</span><span class="token token" style="color:#89ddff">,</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>k</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">*</span><span>sizeof</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">*</span><span>key_cache_row</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>memcpy</span><span class="token token" style="color:#89ddff">(</span><span>value_cache_row</span><span class="token token" style="color:#89ddff">,</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>v</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">*</span><span>sizeof</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">*</span><span>value_cache_row</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span></code></div></pre>
<h3>Doing self-attention</h3>
<p>Formula</p>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>\begin</span><span class="token token" style="color:#89ddff">{</span><span>align</span><span class="token token" style="color:#89ddff">}</span><span> 
</span><span>out </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>QK</span><span class="token token" style="color:#89ddff">^</span><span class="token token" style="color:#89ddff">{</span><span>T</span><span class="token token" style="color:#89ddff">}</span><span class="token token" style="color:#89ddff">)</span><span>\</span><span class="token token" style="color:#89ddff">;</span><span>V</span><span class="token token" style="color:#89ddff">/</span><span>\sqrt</span><span class="token token" style="color:#89ddff">{</span><span>d</span><span class="token token" style="color:#89ddff">}</span><span> \\
</span><span>where\</span><span class="token token" style="color:#89ddff">;</span><span>\</span><span class="token token" style="color:#89ddff">;</span><span>\</span><span class="token token" style="color:#89ddff">;</span><span> Q</span><span class="token token" style="color:#89ddff">=</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">,</span><span>dim</span><span class="token token" style="color:#89ddff">)</span><span> \</span><span class="token token" style="color:#89ddff">;</span><span>\</span><span class="token token" style="color:#89ddff">;</span><span> K</span><span class="token token" style="color:#89ddff">=</span><span class="token token" style="color:#89ddff">(</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span>N</span><span class="token token" style="color:#89ddff">)</span><span> \</span><span class="token token" style="color:#89ddff">;</span><span>\</span><span class="token token" style="color:#89ddff">;</span><span> V</span><span class="token token" style="color:#89ddff">=</span><span class="token token" style="color:#89ddff">(</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span>N</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>\end</span><span class="token token" style="color:#89ddff">{</span><span>align</span><span class="token token" style="color:#89ddff">}</span></code></div></pre>
<p>In above $N$ is <code class="">pos</code> (current length of the generated text)</p>
<p>This part of the code becomes easy if you remember that <code class="">s-&gt;q</code>, <code class="">s-&gt;k</code> when viewed in terms of heads are of shape <code class="">(dim, n_heads, head_dim)</code> &amp; <code class="">key_cache</code>&#x27;s are <code class="">(seq_length, n_heads, head_dim)</code>. Lets go over the code</p>
<ol>
<li><code class="">int h</code> is the current head count. Lets look at each line one by one<!-- -->
<ol>
<li><code class="">q = s-&gt;q + h*head_size</code> :  Gets pointer to start of $h^{th}$ head. Remember <code class="">formula-1</code>. Matrix is of size <code class="">(dim, n_heads, head_dim)</code> we need <code class="">s-&gt;q[0][h][0]</code> which is <code class="">0*n_heads*head_dim + h*head_dim + 0</code> which is <code class="">h*head_size</code>.</li>
<li><code class="">att = s-&gt;att + h * p-&gt;seq_len</code>: We will store attention in <code class="">s-&gt;attn</code> run state variable.</li>
<li>For each position (<code class="">pos</code> is 2 currently if you go back to &quot;fox&quot;, &quot;jumps&quot;, &quot;over&quot; example)
1.To get  $l^{th}$ layer, $t^{th}$ position &amp; $h^{th}$ head we do <code class="">s-&gt;key_cache + l*seq_length*dim + t*n_heads*head_dim + h*head_dim</code> . Since <code class="">loff</code> defined before is already <code class="">l*seq_length*dim</code>. Final offset is <code class="">loff + t*n_heads*head_dim + h*head_size</code> since <code class="">n_heads*head_dim=dim</code> we get offset as <code class="">loff + t*dim + h*head_size</code>.</li>
<li>We now have <code class="">q</code> <code class="">(head_size,)</code>, <code class="">k</code> <code class="">(head_size,)</code>  &amp; <code class="">att</code> <code class="">(seq_length,)</code>. We can calculate self-attention score for $h^{th}$ head at position $t$.  We sum this over all the heads &amp; positions till now.</li>
</ol>
</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>	</span><span class="token token" style="color:#ffcb6b">int</span><span> h</span><span class="token token" style="color:#89ddff">;</span><span>        
</span><span>	</span><span class="token token" style="color:#616161">#pragma omp parallel for private(h)</span><span>
</span><span>	</span><span class="token token" style="color:#c792ea">for</span><span> </span><span class="token token" style="color:#89ddff">(</span><span>h </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span> h </span><span class="token token" style="color:#89ddff">&lt;</span><span> p</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>n_heads</span><span class="token token" style="color:#89ddff">;</span><span> h</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>	</span><span class="token token" style="color:#89ddff">//</span><span> get the query vector </span><span class="token token" style="color:#c792ea">for</span><span> this head
</span><span>	</span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> q </span><span class="token token" style="color:#89ddff">=</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>q </span><span class="token token" style="color:#89ddff">+</span><span> h </span><span class="token token" style="color:#89ddff">*</span><span> head_size</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>	</span><span class="token token" style="color:#89ddff">//</span><span> attention scores </span><span class="token token" style="color:#c792ea">for</span><span> this head
</span><span>	</span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> att </span><span class="token token" style="color:#89ddff">=</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>att </span><span class="token token" style="color:#89ddff">+</span><span> h </span><span class="token token" style="color:#89ddff">*</span><span> p</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>seq_len</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>	</span><span class="token token" style="color:#89ddff">//</span><span> iterate over </span><span class="token token" style="color:#ffcb6b">all</span><span> timesteps</span><span class="token token" style="color:#89ddff">,</span><span> including the current one
</span><span>	</span><span class="token token" style="color:#c792ea">for</span><span> </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> t </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span> t </span><span class="token token" style="color:#89ddff">&lt;=</span><span> pos</span><span class="token token" style="color:#89ddff">;</span><span> t</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>		</span><span class="token token" style="color:#89ddff">//</span><span> get the key vector </span><span class="token token" style="color:#c792ea">for</span><span> this head </span><span class="token token" style="color:#c792ea">and</span><span> at this timestep
</span><span>		</span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> k </span><span class="token token" style="color:#89ddff">=</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>key_cache </span><span class="token token" style="color:#89ddff">+</span><span> loff </span><span class="token token" style="color:#89ddff">+</span><span> t </span><span class="token token" style="color:#89ddff">*</span><span> dim </span><span class="token token" style="color:#89ddff">+</span><span> h </span><span class="token token" style="color:#89ddff">*</span><span> head_size</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>		</span><span class="token token" style="color:#89ddff">//</span><span> calculate the attention score </span><span class="token token" style="color:#c792ea">as</span><span> the dot product of q </span><span class="token token" style="color:#c792ea">and</span><span> k
</span><span>		</span><span class="token token" style="color:#ffcb6b">float</span><span> score </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">.</span><span>0f</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>		</span><span class="token token" style="color:#c792ea">for</span><span> </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> i </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span> i </span><span class="token token" style="color:#89ddff">&lt;</span><span> head_size</span><span class="token token" style="color:#89ddff">;</span><span> i</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>			score </span><span class="token token" style="color:#89ddff">+=</span><span> q</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span> </span><span class="token token" style="color:#89ddff">*</span><span> k</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>		</span><span class="token token" style="color:#89ddff">}</span><span>
</span><span>		score </span><span class="token token" style="color:#89ddff">/=</span><span> sqrtf</span><span class="token token" style="color:#89ddff">(</span><span>head_size</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>		</span><span class="token token" style="color:#89ddff">//</span><span> save the score to the attention </span><span class="token token" style="color:#ffcb6b">buffer</span><span>
</span><span>		att</span><span class="token token" style="color:#89ddff">[</span><span>t</span><span class="token token" style="color:#89ddff">]</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> score</span><span class="token token" style="color:#89ddff">;</span></code></div></pre>
<ol start="2">
<li><code class="">attn</code> obtained above is of shape <code class="">(seq_length, )</code>. Next we multiply it with <code class="">v</code> which is <code class="">(seq_length, dim)</code>. Remember the below loop is inside the <code class="">for (h = 0; h &lt; p-&gt;n_heads; h++)</code> that started in previous section.</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#89ddff">//</span><span> softmax the scores to get attention weights</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#c792ea">from</span><span> </span><span class="token token" style="color:#fd9170">0.</span><span class="token token" style="color:#89ddff">.</span><span>pos inclusively
</span><span>softmax</span><span class="token token" style="color:#89ddff">(</span><span>att</span><span class="token token" style="color:#89ddff">,</span><span> pos </span><span class="token token" style="color:#89ddff">+</span><span> </span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span>
<span></span><span class="token token" style="color:#89ddff">//</span><span> weighted </span><span class="token token" style="color:#ffcb6b">sum</span><span> of the values</span><span class="token token" style="color:#89ddff">,</span><span> store back into xb
</span><span></span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> xb </span><span class="token token" style="color:#89ddff">=</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb </span><span class="token token" style="color:#89ddff">+</span><span> h </span><span class="token token" style="color:#89ddff">*</span><span> head_size</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>memset</span><span class="token token" style="color:#89ddff">(</span><span>xb</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">,</span><span> head_size </span><span class="token token" style="color:#89ddff">*</span><span> sizeof</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span></span><span class="token token" style="color:#c792ea">for</span><span> </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> t </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span> t </span><span class="token token" style="color:#89ddff">&lt;=</span><span> pos</span><span class="token token" style="color:#89ddff">;</span><span> t</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>	</span><span class="token token" style="color:#89ddff">//</span><span> get the value vector </span><span class="token token" style="color:#c792ea">for</span><span> this head </span><span class="token token" style="color:#c792ea">and</span><span> at this timestep
</span><span>	</span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">*</span><span> v </span><span class="token token" style="color:#89ddff">=</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>value_cache </span><span class="token token" style="color:#89ddff">+</span><span> loff </span><span class="token token" style="color:#89ddff">+</span><span> t </span><span class="token token" style="color:#89ddff">*</span><span> dim </span><span class="token token" style="color:#89ddff">+</span><span> h </span><span class="token token" style="color:#89ddff">*</span><span> head_size</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>	</span><span class="token token" style="color:#89ddff">//</span><span> get the attention weight </span><span class="token token" style="color:#c792ea">for</span><span> this timestep
</span><span>	</span><span class="token token" style="color:#ffcb6b">float</span><span> a </span><span class="token token" style="color:#89ddff">=</span><span> att</span><span class="token token" style="color:#89ddff">[</span><span>t</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>	</span><span class="token token" style="color:#89ddff">//</span><span> accumulate the weighted value into xb
</span><span>	</span><span class="token token" style="color:#c792ea">for</span><span> </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> i </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span> i </span><span class="token token" style="color:#89ddff">&lt;</span><span> head_size</span><span class="token token" style="color:#89ddff">;</span><span> i</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>		xb</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span> </span><span class="token token" style="color:#89ddff">+=</span><span> a </span><span class="token token" style="color:#89ddff">*</span><span> v</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>	</span><span class="token token" style="color:#89ddff">}</span><span>
</span><span></span><span class="token token" style="color:#89ddff">}</span></code></div></pre>
<hr/>
<h3>Feed Forward &amp; Classifier</h3>
<ol>
<li>To complete attention module, we need to multiply with $O$ which we do in first line. Next line <code class="">accum</code> adds input which comes from skip layer (red arrow) &amp; output of attention. Followed by normalization.</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#89ddff">//</span><span> final matmul to get the output of the attention
</span><span>matmul</span><span class="token token" style="color:#89ddff">(</span><span>s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb2</span><span class="token token" style="color:#89ddff">,</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb</span><span class="token token" style="color:#89ddff">,</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>wo </span><span class="token token" style="color:#89ddff">+</span><span> l</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span></span><span class="token token" style="color:#89ddff">//</span><span> residual connection back into x
</span><span>accum</span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">,</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb2</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span></span><span class="token token" style="color:#89ddff">//</span><span> ffn rmsnorm
</span><span>rmsnorm</span><span class="token token" style="color:#89ddff">(</span><span>s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb</span><span class="token token" style="color:#89ddff">,</span><span> x</span><span class="token token" style="color:#89ddff">,</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>rms_ffn_weight </span><span class="token token" style="color:#89ddff">+</span><span> l</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span></code></div></pre>
<p><img src="https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/accum.png" alt="Accum"/></p>
<ol start="2">
<li>Next we calculate the FFN output which is</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>out </span><span class="token token" style="color:#89ddff">=</span><span> W_</span><span class="token token" style="color:#89ddff">{</span><span class="token token" style="color:#fd9170">3</span><span class="token token" style="color:#89ddff">}</span><span>\</span><span class="token token" style="color:#89ddff">;</span><span>\sigma </span><span class="token token" style="color:#89ddff">(</span><span>W_</span><span class="token token" style="color:#89ddff">{</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">}</span><span>X</span><span class="token token" style="color:#89ddff">*</span><span>W_</span><span class="token token" style="color:#89ddff">{</span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">}</span><span>X</span><span class="token token" style="color:#89ddff">)</span></code></div></pre>
<p>$\sigma$ is <code class="">silu</code> <a href="https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html">activation</a>.</p>
<p><img src="https://pytorch.org/docs/stable/_images/SiLU.png" alt="Silu"/></p>
<p>This portion is self explanatory</p>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#89ddff">//</span><span> Now </span><span class="token token" style="color:#c792ea">for</span><span> FFN </span><span class="token token" style="color:#c792ea">in</span><span> PyTorch we have</span><span class="token token" style="color:#89ddff">:</span><span> self</span><span class="token token" style="color:#89ddff">.</span><span>w2</span><span class="token token" style="color:#89ddff">(</span><span>F</span><span class="token token" style="color:#89ddff">.</span><span>silu</span><span class="token token" style="color:#89ddff">(</span><span>self</span><span class="token token" style="color:#89ddff">.</span><span>w1</span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">*</span><span> self</span><span class="token token" style="color:#89ddff">.</span><span>w3</span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span></span><span class="token token" style="color:#89ddff">//</span><span> first calculate self</span><span class="token token" style="color:#89ddff">.</span><span>w1</span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#c792ea">and</span><span> self</span><span class="token token" style="color:#89ddff">.</span><span>w3</span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>matmul</span><span class="token token" style="color:#89ddff">(</span><span>s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>hb</span><span class="token token" style="color:#89ddff">,</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb</span><span class="token token" style="color:#89ddff">,</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>w1 </span><span class="token token" style="color:#89ddff">+</span><span> l</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">*</span><span>hidden_dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">,</span><span> hidden_dim</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>matmul</span><span class="token token" style="color:#89ddff">(</span><span>s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>hb2</span><span class="token token" style="color:#89ddff">,</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb</span><span class="token token" style="color:#89ddff">,</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>w3 </span><span class="token token" style="color:#89ddff">+</span><span> l</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">*</span><span>hidden_dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">,</span><span> hidden_dim</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span></span><span class="token token" style="color:#89ddff">//</span><span> F</span><span class="token token" style="color:#89ddff">.</span><span>silu</span><span class="token token" style="color:#89ddff">;</span><span> silu</span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">=</span><span>x</span><span class="token token" style="color:#89ddff">*</span><span></span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">,</span><span>where </span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#c792ea">is</span><span> the logistic sigmoid
</span><span></span><span class="token token" style="color:#c792ea">for</span><span> </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> i </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span> i </span><span class="token token" style="color:#89ddff">&lt;</span><span> hidden_dim</span><span class="token token" style="color:#89ddff">;</span><span> i</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>	s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>hb</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>hb</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span> </span><span class="token token" style="color:#89ddff">*</span><span> </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">.</span><span>0f </span><span class="token token" style="color:#89ddff">/</span><span> </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">.</span><span>0f </span><span class="token token" style="color:#89ddff">+</span><span> expf</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">-</span><span>s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>hb</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span></span><span class="token token" style="color:#89ddff">}</span><span>
</span><span></span><span class="token token" style="color:#89ddff">//</span><span> elementwise multiply </span><span class="token token" style="color:#c792ea">with</span><span> w3</span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span></span><span class="token token" style="color:#c792ea">for</span><span> </span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">int</span><span> i </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">;</span><span> i </span><span class="token token" style="color:#89ddff">&lt;</span><span> hidden_dim</span><span class="token token" style="color:#89ddff">;</span><span> i</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">+</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#89ddff">{</span><span>
</span><span>	s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>hb</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span> </span><span class="token token" style="color:#89ddff">=</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>hb</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span> </span><span class="token token" style="color:#89ddff">*</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>hb2</span><span class="token token" style="color:#89ddff">[</span><span>i</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span></span><span class="token token" style="color:#89ddff">}</span><span>
</span><span></span><span class="token token" style="color:#89ddff">//</span><span> final matmul to get the output of the ffn
</span><span></span><span class="token token" style="color:#89ddff">//</span><span>memcpy</span><span class="token token" style="color:#89ddff">(</span><span>tmp_w_hid</span><span class="token token" style="color:#89ddff">,</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>w2 </span><span class="token token" style="color:#89ddff">+</span><span> l</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">*</span><span>hidden_dim</span><span class="token token" style="color:#89ddff">,</span><span> hidden_dim</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">*</span><span>sizeof</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">float</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span>matmul</span><span class="token token" style="color:#89ddff">(</span><span>s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb</span><span class="token token" style="color:#89ddff">,</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>hb</span><span class="token token" style="color:#89ddff">,</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>w2 </span><span class="token token" style="color:#89ddff">+</span><span> l</span><span class="token token" style="color:#89ddff">*</span><span>dim</span><span class="token token" style="color:#89ddff">*</span><span>hidden_dim</span><span class="token token" style="color:#89ddff">,</span><span> hidden_dim</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span></code></div></pre>
<ol start="3">
<li>The last line is another accum (2nd skip layer in above diagram)</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>accum</span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">,</span><span> s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>xb</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span></code></div></pre>
<hr/>
<h3>Final Classifier</h3>
<p>After running above module for all layers, we get an embedding of shape <code class="">(dim,)</code>. We need to convert this into a vector of shape <code class="">(vocab,)</code> whose each entry tells us what is the score for that word to be next token.</p>
<ol>
<li>Before multiplying with classifier matrix (<code class="">w-&gt;wcls</code>) we normalize our embedding. The scores our saved in <code class="">s-&gt;logits</code></li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#89ddff">//</span><span> final rmsnorm
</span><span>rmsnorm</span><span class="token token" style="color:#89ddff">(</span><span>x</span><span class="token token" style="color:#89ddff">,</span><span> x</span><span class="token token" style="color:#89ddff">,</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>rms_final_weight</span><span class="token token" style="color:#89ddff">,</span><span> dim</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span><span>
</span><span></span><span class="token token" style="color:#89ddff">//</span><span> classifier into logits
</span><span>matmul</span><span class="token token" style="color:#89ddff">(</span><span>s</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>logits</span><span class="token token" style="color:#89ddff">,</span><span> x</span><span class="token token" style="color:#89ddff">,</span><span> w</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>wcls</span><span class="token token" style="color:#89ddff">,</span><span> p</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>dim</span><span class="token token" style="color:#89ddff">,</span><span> p</span><span class="token token" style="color:#89ddff">-</span><span class="token token" style="color:#89ddff">&gt;</span><span>vocab_size</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">;</span></code></div></pre>
<hr/>
<h3>The end</h3>
<p>Once we get <code class="">s-&gt;logits</code> we sample next token (do this until we get <code class="">seq_length</code> tokens). This has already been covered in &quot;Forward Loop &amp; sampling in main&quot; section.  Congratulations! now you know how LLMs work &amp; how to code them in C. If you now want to know how to code them in Python know, refer to <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py">modelling_llama.py</a></p>
<p>Here is a picture of a cat :)</p>
<p><img src="https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/cat.jpg" alt="Cat"/></p></div><div class="Home_main3__oLarL"></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"llama2_dummies","contentHtml":"\n# llama2.c for Dummies\n\n### Purpose\nThis repo is line by line walk through of the inference file in [llama2.c](http://github.com/karpathy/llama2.c). Its very verbose \u0026 intended for beginners.\n\nYou will need some familiarity with transformers architecture. If you are a complete novice refer to this excellent [blog](https://jalammar.github.io/illustrated-transformer/) first.\n\n----\n\n### Prerequisites\n\n1. Transformer architecture: 3 components\n\t1. Embedding (1 matmul)\n\t2. Layers: matmul with Q, K , V, O and feed forward weights: W1, W2 \u0026 W3. (7 matmul)\n\t3. Classifier: In our case the classifier is just matmul of `(vocab,768) x (768,1)` . Basically giving us what is the probability of each next token. (1 matmul)\n\n\n![Architechure ](https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/arch.png)\n\n\n\n## Code walkthrough\n\nCode has 3 parts, structs, functions \u0026 read logic in `main()` we will take a look at structs first, then go to main() and then cover the important functions.\n\n**PS: The code was taken from commit 4e23ad83. The original repo might be different as it gets newer commits.** But 99% of the logic should remain the same :) \n\n### Part 1: Structs\n\nWe define 3 structs for storing model config, model weights \u0026 to store intermediate values (run state) during forward pass\n\n1. **Config struct**: Defines the transformer model.\n\t1. `n_layers` , `vocab_size`  : no. of layers (e.g. llama-2 has 32 layers/BERT-base has 12 layers) \u0026 no. of tokens in our vocabulary (this is usually 30k for english languages)\n\t2. `dim` and `hidden_dim` : Define shape of Q, K, V \u0026 O `(dim,dim)` and W1, W2 `(dim, hidden_dim)`\u0026 W3 `(hidden_dim, dim)` \n\t3. `n_heads` : Number of heads for query(Q). If `n_heads=12` then matrix `Q=(768,768)` behaves/viewed as `(768, 768/12,768)`\n\t4. `n_kv_heads` : Number of heads for K \u0026 V. **Why are these different from above?** : Read [multi query paper](https://arxiv.org/pdf/1911.02150.pdf)\n\t5. `seq_len` : No. of tokens we will generate\n```c\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be \u003c query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n```\n\n---\n\n2. **Weight struct** for llama. This is our pytorch `ffn=nn.Linear(...)` counterpart. \n\t1. Why are they `float*`  ? Because all matrices are just 1d flattened array. See below diagram\n\t2. code is self explanatory with shapes commented.   `rms_`  are weights used for normalization \u0026 `freq_cis_` are for [RoPE embedding](https://arxiv.org/pdf/2104.09864.pdf). We will look at `RoPE` in detail ahead.\n\t3. `wcls` is the final classifier. Matrix of size `(vocab, dim)` that maps final embedding from a vector to probability for each token in vocab.\n```c\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls\n    float* wq; // (layer, dim, dim)\n    float* wk; // (layer, dim, dim)\n    float* wv; // (layer, dim, dim)\n    float* wo; // (layer, dim, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // freq_cis for RoPE relatively positional embeddings\n    float* freq_cis_real; // (seq_len, dim/2)\n    float* freq_cis_imag; // (seq_len, dim/2)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n```\n\n![Array](https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/arr.png)\n\n---\n\n3. Intermediate activations (Run state)\n\t1. During forward pass we need to store intermediate values, e.g. output of matmul or output after norm. Will take a look at all variables later\n\t2. `key_cahce` and `value_cache` store the key, value outputs of previous tokens. e.g. during inference if the 5th token is being generated, this will store `key`, `value` of the previous 4.\n\n```c\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n```\n\n---\n\nWe will take a look at functions as we encounter them. For now lets see the logic inside `main()` \n\n### Part 2: Main (Can skip this part if you are only interested in [forward logic](#actual-forward-pass) )\n\n1. Get command line arguments. Nothing interesting.  Currently you can call `run.c` with\n\t1. `./run llama2_7b.bin`\n\t2. `./run llama2_7b.bin 0.1` -\u003e with temperature\n\t3. `./run llama2_7b.bin 0.1 100` -\u003e with temperature \u0026 steps (no. of output tokens generated)\n \n2. Declare `config` \u0026 `weights` in the end\n```c\nint main(int argc, char *argv[]) {\n    // poor man's C argparse\n    char *checkpoint = NULL;  // e.g. out/model.bin\n    float temperature = 0.9f; // e.g. 1.0, or 0.0\n    int steps = 256;          // max number of steps to run for, 0: use seq_len\n    // 'checkpoint' is necessary arg\n    if (argc \u003c 2) {\n        printf(\"Usage: %s \u003ccheckpoint_file\u003e [temperature] [steps]\\n\", argv[0]);\n        return 1;\n    }\n    if (argc \u003e= 2) {\n        checkpoint = argv[1];\n    }\n    if (argc \u003e= 3) {\n        // optional temperature. 0.0 = (deterministic) argmax sampling. 1.0 = baseline\n        temperature = atof(argv[2]);\n    }\n    if (argc \u003e= 4) {\n        steps = atoi(argv[3]);\n    }\n\t// seed rng with time. if you want deterministic behavior use temperature 0.0\n    srand((unsigned int)time(NULL)); \n    // read in the model.bin file\n    Config config;\n    TransformerWeights weights;\n```\n\n2. Reading `checkpoint` file. \n\t1. If you are familiar with PyTorch. Usually  `config.json` \u0026 `model.bin` are separate (we load weights like a dictionary). But here `train.py` saves everything in one `.bin`  file in a specific format. This specific format allows us to easily read config \u0026 then each weight one by one.\n\n\tDetails\n\t1.  `shared_weights` : Should input embedding matrix \u0026 output classifier matrix be same?  \n\t2. Next load into `weights`. Get file size via `file_size = ftell(file);` Unlike vanilla PyTorch inference we **don't** load all weights into RAM. Instead we call `mmap(..)` to allocate RAM memory when we want lazily. For more detail [read](https://stackoverflow.com/questions/5877797/how-does-mmap-work)\n\t3. Finally  call `checkpoint_init_weights`  (snippet of function below). Here we map our weight pointers to correct address returned by `mmap`. Since we already read config we offset for it in line  `float* weights_ptr = data + sizeof(Config)/sizeof(float);`\n```c\nvoid checkpoint_init_weights(TransformerWeights *w, Config* p, float* f, int shared_weights){\nfloat* ptr = f;\nw-\u003etoken_embedding_table = ptr;\nptr += p-\u003evocab_size * p-\u003edim;\nw-\u003erms_att_weight = ptr;\n.......\n}\n```\n\nOriginal code we are talking about in above section\n```c\n    int fd = 0;\n    float* data = NULL;\n    long file_size;\n    {\n        FILE *file = fopen(checkpoint, \"rb\");\n        if (!file) {\n            printf(\"Unable to open the checkpoint file %s!\\n\", checkpoint);\n            return 1;\n        } \n\t    // read in the config header\n        if(fread(\u0026config, sizeof(Config), 1, file) != 1) { return 1; }\n        // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n        int shared_weights = config.vocab_size \u003e 0 ? 1 : 0;\n        config.vocab_size = abs(config.vocab_size);\n        // figure out the file size\n        fseek(file, 0, SEEK_END); // move file pointer to end of file\n        file_size = ftell(file); // get the file size, in bytes\n        fclose(file);\n        \n        // memory map the Transformer weights into the data pointer\n        fd = open(checkpoint, O_RDONLY); // open in read only mode\n        if (fd == -1) { printf(\"open failed!\\n\"); return 1; }\n        data = mmap(NULL, file_size, PROT_READ, MAP_PRIVATE, fd, 0);\n        if (data == MAP_FAILED) { printf(\"mmap failed!\\n\"); return 1; }\n        float* weights_ptr = data + sizeof(Config)/sizeof(float);\n        checkpoint_init_weights(\u0026weights, \u0026config, weights_ptr, shared_weights);\n    }\n```\n\n---\n\n3. Reading vocab file -\u003e Mostly straightforward, only few details\n\t1. `vocab` is `char**` since each token is a string \u0026 `vocab` is a list of tokens.\n\t2. For loop over `vocab_size` \u0026 read each token\n```c\n// right now we cannot run for more than config.seq_len steps\n    if (steps \u003c= 0 || steps \u003e config.seq_len) { steps = config.seq_len; }\n    // read in the tokenizer.bin file\n    char** vocab = (char**)malloc(config.vocab_size * sizeof(char*));\n    {\n        FILE *file = fopen(\"tokenizer.bin\", \"rb\");\n        if (!file) {\n            printf(\"Unable to open the tokenizer file tokenizer.bin! Run \"\n            \"python tokenizer.py to convert tokenizer.model -\u003e tokenizer.bin\\n\");\n            return 1;\n        }\n        int len;\n        for (int i = 0; i \u003c config.vocab_size; i++) {\n            if(fread(\u0026len, sizeof(int), 1, file) != 1) { return 1; }\n            vocab[i] = (char *)malloc(len + 1);\n            if(fread(vocab[i], len, 1, file) != 1) { return 1; }\n            vocab[i][len] = '\\0'; // add the string terminating token\n        }\n        fclose(file);\n    }\n```\n\n---\n\n#### Forward Loop \u0026 sampling in main (Go to [important part](#actual-forward-pass))\n\n1. Allocate memory for run state/intermediate values. The first `token` we pass into our model is BOS token (\"Beginning of Statement\") who's vocab index is `1`. \n```c\n    RunState state;\n    malloc_run_state(\u0026state, \u0026config);\n    \n    // the current position we are in\n    long start = time_in_ms();\n    int next;\n    int token = 1; // 1 = BOS token in Llama-2 sentencepiece\n    int pos = 0;\n    printf(\"\u003cs\u003e\\n\"); // explicit print the initial BOS token (=1), stylistically symmetric\n```\n\n![Luke](https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs//luke.jpeg)\n\n2. Forward loop:\n\t1. `transformer(token, pos, \u0026config, \u0026state, \u0026weights);` stores classifier score of each token as being the next token in sequence inside `state.logits`.(contents of `transformer` function convered in next section). \n\t2. Next we sample. **Why we need sampling \u0026 how to do it?**\n \t\t- Lets say you want AI to complete dialogues of a movie \u0026 your input is _\"Luke, I am your\"_ . Now `llama` gives you score for each token to be the next word. So e.g. assume our tokens are `[\"Apple\", \"Football\", \"Father\", \"Brother\"]` \u0026 llama gives them scores of `[0.3, 0.1, 0.9, 0.7]`. Now to pick the next token, either we take maximum (`\"Father\"` with score 0.9) or we sample tokens with a probability proportional to thier score, this way we can get more diversity(very important in today's world ) in our prediction. \n\n \t3. Lets discuss some more details: If `temperature=0` then its max sampling.  For `temperate\u003e0` we convert `state.logits` into probabilities using softmax \u0026 store back in `state.logits`. The `sample(..)` function returns a token sampled from the `state.logits` probability distribution. Read more [here](https://web.mit.edu/urban_or_book/www/book/chapter7/7.1.3.html) \n\t5. The token generated `next` becomes the next input token in line `token=next`. \n```c\nwhile (pos \u003c steps) {\n        // forward the transformer to get logits for the next token\n        transformer(token, pos, \u0026config, \u0026state, \u0026weights);\n        // sample the next token\n        if(temperature == 0.0f) {\n            // greedy argmax sampling\n            next = argmax(state.logits, config.vocab_size);\n        } else {\n            // apply the temperature to the logits\n            for (int q=0; q\u003cconfig.vocab_size; q++) { state.logits[q] /= temperature; }\n            // apply softmax to the logits to get the probabilities for next token\n            softmax(state.logits, config.vocab_size);\n            // we now want to sample from this distribution to get the next token\n            next = sample(state.logits, config.vocab_size);\n        }\n        printf(\"%s\", vocab[next]);\n        fflush(stdout);\n\n        // advance forward\n        token = next;\n        pos++;\n    }\n```\n---\n\n### Actual Forward pass\n\nDetails of `transformer(token, pos, \u0026config, \u0026state, \u0026weights);` called from `main()`\n\nSection below uses 2d/3d array indexing extensively. We cover it briefly here to make life easier\n\n1. If matrix `float* mat` is of size `(dim1, dim2, dim3)` then pointer to access `mat[l][i][j]` is `dim2*dim3*l + dim3*i + j;` -  This is `formula-1` we will refer to this often later. Read [link](https://www.learncpp.com/cpp-tutorial/pointer-arithmetic-and-array-indexing/) if you are confused\n\nHow to view matrices in terms of head?\n1.  K (key) `float* wk` is a matrix defined as shape `(layer, dim, dim)` when viewed in terms of heads is `(layer, dim, n_heads, head_dim)`\n---\n\n1. Convenience variables. Nothing interesting apart from copying the embedding of `token` into `s-\u003exb` using `memcpy`. Why not use `float* content_row` itself? Because  `s-\u003exb` is going to change \u0026 using `content_row` will change model weights.\n```c\nvoid transformer(int token, int pos, Config* p, RunState* s, TransformerWeights* w) {\n    // a few convenience variables\n    float *x = s-\u003ex;\n    int dim = p-\u003edim;                  \n    int hidden_dim =  p-\u003ehidden_dim;  \n    int head_size = dim / p-\u003en_heads; \n    float* content_row = \u0026(w-\u003etoken_embedding_table[token * dim]);\n    // copy the token embedding into x\n    memcpy(x, content_row, dim*sizeof(*x)); \n```\n\n---\n**RoPE** : Rotary Positional Embeddings \n- Formulation:  Transforms feature pairs by rotating it in 2D plane.\n\te.g. If your vector is `[0.8, 0.5, -0.1, 0.3]` we group them into pairs: `[[0.8,-0.1], [0.5, 0.3]` and rotate by some angle $\\theta$. This $\\theta$ is ~~part of the weights \u0026 is learned during training~~ $\\theta$ is fixed from the start (its not learnable). In the paper the value of $\\theta_{i}$ is $10000^{2(i-1)/d}$ \n\nRoPE  Formula (For 2 features grouped into a pair) is below. $m$ is the index of the pair. $\\theta$ is a learned parameter that we load from `.bin` file\n\n$$\n  \\left[ {\\begin{array}{ccccc}\n   x_{m}^{i} \u0026 x_{m}^{j} \\\\\n  \\end{array} } \\right] * \\left[ {\\begin{array}{ccccc}\n   cos(m\\theta_{m}) \u0026 -sin(m\\theta_{m}) \\\\\n   sin(m\\theta_{m}) \u0026 cos(m\\theta_{m}) \\\\\n  \\end{array} } \\right]\n$$\n\nOur example pair `[[0.8,-0.1], [0.5, 0.3]`  will be transformed like below. Keep in mind for the first pair `[0.8, 0.1]` $m=0$ since (therefore $sin(0)=0$). And for 2nd pair `m=1`\n\n$$\n  \\left[ {\\begin{array}{ccccc}\n   0.8 \u0026 -0.1 \\\\\n  \\end{array} } \\right] * \\left[ {\\begin{array}{ccccc}\n   1 * 1 \u0026 -0.0 * 1 \\\\\n   0.0 * 1 \u0026 1.0 * 1 \\\\\n  \\end{array} } \\right] =    \\left[ {\\begin{array}{ccccc}\n   0.8 \u0026 -0.1 \\\\\n  \\end{array} } \\right]\n$$\n\n$$\n  \\left[ {\\begin{array}{ccccc}\n   0.5 \u0026 0.3 \\\\\n  \\end{array} } \\right] * \\left[ {\\begin{array}{ccccc}\n   0.86 * 1 \u0026 -0.5 * 1 \\\\\n   0.5 * 1 \u0026 0.86 * 1 \\\\\n  \\end{array} } \\right] =    \\left[ {\\begin{array}{ccccc}\n   0.58 \u0026 0.08 \\\\\n  \\end{array} } \\right]\n$$\n\nCombining both, the output is `[[0.8, 0.1], [0.58, 0.08]]` now **un-pairing** them will give us `[0.8, 0.58, 0.1, 0.08]`\nSo `RoPE` transformed `[0.8, 0.5, -0.1, 0.3]` into `[0.8, 0.58, 0.1, 0.08]`. Keep in mind if a feature is of `dim=768` then there are half of it **384** learnable $\\theta$'s. \n\n**Back to code**\n1. We get $\\theta$ for current position (`pos` is our $m$).  `freq_cis_real_row` is $cos(m\\theta)$ and `freq_cis_imag_row` is $sin(m\\theta)$.\n```c\n    // pluck out the \"pos\" row of freq_cis_real and freq_cis_imag66\n    float* freq_cis_real_row = w-\u003efreq_cis_real + pos * head_size / 2;\n    float* freq_cis_imag_row = w-\u003efreq_cis_imag + pos * head_size / 2;\n```\n\n2. Iterate over layers. Apply `rmsnorm` to input of the layer.  `rmsnorm` function calculates the below\n\n```math\nout\\; = \\;  (x*g*n)/\\sum_{i} \\sqrt{x_{i}^{2}} \n```\nwhere $x$ is input, $g$ is learnable parameter (`w-\u003erms_attn_weight` below) \u0026 $n$ is `dim`.\n\n`matmul` does matrix mult of a 2d matrix with a 1d matrix. `(A, B) x (A,)`. The implementation is trivial (we cover this at very end). We multiply Q,K,V with `s-\u003exb` (output of `rmsnorm`) and store output in `s-\u003eq`, `s-\u003ek` ..\n```c\nfor(int l = 0; l \u003c p-\u003en_layers; l++) {\n// attention rmsnorm\n\trmsnorm(s-\u003exb, x, w-\u003erms_att_weight + l*dim, dim);\n\t\n\t// qkv matmuls for this position\n\tmatmul(s-\u003eq, s-\u003exb, w-\u003ewq + l*dim*dim, dim, dim);\n\tmatmul(s-\u003ek, s-\u003exb, w-\u003ewk + l*dim*dim, dim, dim);\n\tmatmul(s-\u003ev, s-\u003exb, w-\u003ewv + l*dim*dim, dim, dim);\n```\n3. Go over each head \u0026 apply the 2-d $cos$/$sin$ transformation we discussed above to `s-\u003eq` and `s-\u003ek`. We do it separately for each head, therefore we take offset of `h*head_size`\n```c\n// apply RoPE rotation to the q and k vectors for each head\n        for (int h = 0; h \u003c p-\u003en_heads; h++) {\n            // get the q and k vectors for this head\n            float* q = s-\u003eq + h * head_size;\n            float* k = s-\u003ek + h * head_size;\n            // rotate q and k by the freq_cis_real and freq_cis_imag\n            for (int i = 0; i \u003c head_size; i+=2) {\n                float q0 = q[i];\n                float q1 = q[i+1];\n                float k0 = k[i];\n                float k1 = k[i+1];\n                float fcr = freq_cis_real_row[i/2];\n                float fci = freq_cis_imag_row[i/2];\n                q[i]   = q0 * fcr - q1 * fci;\n                q[i+1] = q0 * fci + q1 * fcr;\n                k[i]   = k0 * fcr - k1 * fci;\n                k[i+1] = k0 * fci + k1 * fcr;\n            }\n        }\n```\n\n\n4. Once we get `q, k, v` for current token, we need to calculate self-attention. Where we multiply query into key.  `k \u0026 v` are only for the current token. We store the `k, v` for all past tokens in `key_cache_row`  \u0026 `value_cache_row`.\n\t- For example, if we have generated the tokens (\"fox\", \"jumps\", \"over\") until now then we already have Q \u0026 V for \"fox\" \u0026 \"jumps\" from previous forward passes stored in our cache. We need not recalculate.\n\t- Since caches store key, query for all layers \u0026 for all tokens (max no.of tokens is `seq_length`) its dimensions are `(layer, seq_length, dim)`. `seq_length` is usually called `context`. \n5. Consider below code in terms of above example. Lets say `seq_length=32` (which means we generate at-most 32 tokens). `pos=2` since \"fox\" is the 3rd token (2nd since python is 0-indexed). \n\t- We already have `layer*(pos-1)*dim` values filled in `s-\u003ekey_cache` We need to fill the key, value of current token \"fox\" into `s-\u003ekey_cache` too before doing self-attention. This is what `memcpy(key_cache_row, s-\u003ek, dim*sizeof(*key_cache_row));` does\n```c\n// save key,value at this time step (pos) to our kv cache\nint loff = l * p-\u003eseq_len * dim; // kv cache layer offset for convenience\nfloat* key_cache_row = s-\u003ekey_cache + loff + pos * dim;\nfloat* value_cache_row = s-\u003evalue_cache + loff + pos * dim;\nmemcpy(key_cache_row, s-\u003ek, dim*sizeof(*key_cache_row));\nmemcpy(value_cache_row, s-\u003ev, dim*sizeof(*value_cache_row));\n```\n\n### Doing self-attention\n\nFormula\n\n```math\n\\begin{align} \nout = (QK^{T})\\;V/\\sqrt{d} \\\\\nwhere\\;\\;\\; Q=(1,dim) \\;\\; K=(dim,N) \\;\\; V=(dim,N)\n\\end{align}\n```\nIn above $N$ is `pos` (current length of the generated text)\n\n\nThis part of the code becomes easy if you remember that `s-\u003eq`, `s-\u003ek` when viewed in terms of heads are of shape `(dim, n_heads, head_dim)` \u0026 `key_cache`'s are `(seq_length, n_heads, head_dim)`. Lets go over the code\n 1. `int h` is the current head count. Lets look at each line one by one\n\t 1. `q = s-\u003eq + h*head_size` :  Gets pointer to start of $h^{th}$ head. Remember `formula-1`. Matrix is of size `(dim, n_heads, head_dim)` we need `s-\u003eq[0][h][0]` which is `0*n_heads*head_dim + h*head_dim + 0` which is `h*head_size`. \n\t 2. `att = s-\u003eatt + h * p-\u003eseq_len`: We will store attention in `s-\u003eattn` run state variable.\n\t 3. For each position (`pos` is 2 currently if you go back to \"fox\", \"jumps\", \"over\" example) \n\t\t\t 1.To get  $l^{th}$ layer, $t^{th}$ position \u0026 $h^{th}$ head we do `s-\u003ekey_cache + l*seq_length*dim + t*n_heads*head_dim + h*head_dim` . Since `loff` defined before is already `l*seq_length*dim`. Final offset is `loff + t*n_heads*head_dim + h*head_size` since `n_heads*head_dim=dim` we get offset as `loff + t*dim + h*head_size`.\n\t1. We now have `q` `(head_size,)`, `k` `(head_size,)`  \u0026 `att` `(seq_length,)`. We can calculate self-attention score for $h^{th}$ head at position $t$.  We sum this over all the heads \u0026 positions till now.\n```c\n\tint h;        \n\t#pragma omp parallel for private(h)\n\tfor (h = 0; h \u003c p-\u003en_heads; h++) {\n\t// get the query vector for this head\n\tfloat* q = s-\u003eq + h * head_size;\n\t// attention scores for this head\n\tfloat* att = s-\u003eatt + h * p-\u003eseq_len;\n\t// iterate over all timesteps, including the current one\n\tfor (int t = 0; t \u003c= pos; t++) {\n\t\t// get the key vector for this head and at this timestep\n\t\tfloat* k = s-\u003ekey_cache + loff + t * dim + h * head_size;\n\t\t// calculate the attention score as the dot product of q and k\n\t\tfloat score = 0.0f;\n\t\tfor (int i = 0; i \u003c head_size; i++) {\n\t\t\tscore += q[i] * k[i];\n\t\t}\n\t\tscore /= sqrtf(head_size);\n\t\t// save the score to the attention buffer\n\t\tatt[t] = score;\n ```\n\n\n2. `attn` obtained above is of shape `(seq_length, )`. Next we multiply it with `v` which is `(seq_length, dim)`. Remember the below loop is inside the `for (h = 0; h \u003c p-\u003en_heads; h++)` that started in previous section.\n\n```c\n// softmax the scores to get attention weights, from 0..pos inclusively\nsoftmax(att, pos + 1);\n\n// weighted sum of the values, store back into xb\nfloat* xb = s-\u003exb + h * head_size;\nmemset(xb, 0, head_size * sizeof(float));\nfor (int t = 0; t \u003c= pos; t++) {\n\t// get the value vector for this head and at this timestep\n\tfloat* v = s-\u003evalue_cache + loff + t * dim + h * head_size;\n\t// get the attention weight for this timestep\n\tfloat a = att[t];\n\t// accumulate the weighted value into xb\n\tfor (int i = 0; i \u003c head_size; i++) {\n\t\txb[i] += a * v[i];\n\t}\n}\n```\n\n\n---\n\n### Feed Forward \u0026 Classifier\n\n1. To complete attention module, we need to multiply with $O$ which we do in first line. Next line `accum` adds input which comes from skip layer (red arrow) \u0026 output of attention. Followed by normalization.\n\n```c\n// final matmul to get the output of the attention\nmatmul(s-\u003exb2, s-\u003exb, w-\u003ewo + l*dim*dim, dim, dim);\n// residual connection back into x\naccum(x, s-\u003exb2, dim);\n// ffn rmsnorm\nrmsnorm(s-\u003exb, x, w-\u003erms_ffn_weight + l*dim, dim);\n```\n![Accum](https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/accum.png)\n\n\n2. Next we calculate the FFN output which is\n```math\nout = W_{3}\\;\\sigma (W_{1}X*W_{2}X)\n```\n$\\sigma$ is `silu` [activation](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html). \n\n\n![Silu](https://pytorch.org/docs/stable/_images/SiLU.png)\n\n\nThis portion is self explanatory \n```c\n// Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n// first calculate self.w1(x) and self.w3(x)\nmatmul(s-\u003ehb, s-\u003exb, w-\u003ew1 + l*dim*hidden_dim, dim, hidden_dim);\nmatmul(s-\u003ehb2, s-\u003exb, w-\u003ew3 + l*dim*hidden_dim, dim, hidden_dim);\n// F.silu; silu(x)=x*(x),where (x) is the logistic sigmoid\nfor (int i = 0; i \u003c hidden_dim; i++) {\n\ts-\u003ehb[i] = s-\u003ehb[i] * (1.0f / (1.0f + expf(-s-\u003ehb[i])));\n}\n// elementwise multiply with w3(x)\nfor (int i = 0; i \u003c hidden_dim; i++) {\n\ts-\u003ehb[i] = s-\u003ehb[i] * s-\u003ehb2[i];\n}\n// final matmul to get the output of the ffn\n//memcpy(tmp_w_hid, w-\u003ew2 + l*dim*hidden_dim, hidden_dim*dim*sizeof(float));\nmatmul(s-\u003exb, s-\u003ehb, w-\u003ew2 + l*dim*hidden_dim, hidden_dim, dim);\n```\n3. The last line is another accum (2nd skip layer in above diagram)\n```c\naccum(x, s-\u003exb, dim);\n```\n\n\n---\n\n### Final Classifier\n\nAfter running above module for all layers, we get an embedding of shape `(dim,)`. We need to convert this into a vector of shape `(vocab,)` whose each entry tells us what is the score for that word to be next token.\n\n1. Before multiplying with classifier matrix (`w-\u003ewcls`) we normalize our embedding. The scores our saved in `s-\u003elogits`\n```c\n// final rmsnorm\nrmsnorm(x, x, w-\u003erms_final_weight, dim);\n// classifier into logits\nmatmul(s-\u003elogits, x, w-\u003ewcls, p-\u003edim, p-\u003evocab_size);\n```\n\n---\n### The end\n\nOnce we get `s-\u003elogits` we sample next token (do this until we get `seq_length` tokens). This has already been covered in \"Forward Loop \u0026 sampling in main\" section.  Congratulations! now you know how LLMs work \u0026 how to code them in C. If you now want to know how to code them in Python know, refer to [modelling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)\n\nHere is a picture of a cat :)  \n\n![Cat](https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/cat.jpg)\n\n\n\n\n\n\n\n\n\n","title":"llama2.c for dummies - Introduction to llama2 in C (Taken directly from my github)","date":"2023-08","name":"Rahul Chand"}},"__N_SSG":true},"page":"/blogs/pages/posts/[id]","query":{"id":"llama2_dummies"},"buildId":"GYLvD-C5QPNQCNXEIuLtF","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>