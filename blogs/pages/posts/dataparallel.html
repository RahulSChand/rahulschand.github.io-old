<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/83683353ad1dcb56.css" as="style"/><link rel="stylesheet" href="/_next/static/css/83683353ad1dcb56.css" data-n-g=""/><link rel="preload" href="/_next/static/css/1feeaed705173f12.css" as="style"/><link rel="stylesheet" href="/_next/static/css/1feeaed705173f12.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-5866cead997b9ace.js" defer=""></script><script src="/_next/static/chunks/main-f37dcad5231e7164.js" defer=""></script><script src="/_next/static/chunks/pages/_app-53e68f2455fa018c.js" defer=""></script><script src="/_next/static/chunks/278-6fddd24af7f03943.js" defer=""></script><script src="/_next/static/chunks/299-abd0aad0309c30e3.js" defer=""></script><script src="/_next/static/chunks/pages/blogs/pages/posts/%5Bid%5D-757ef647e5f57c03.js" defer=""></script><script src="/_next/static/JnDgV02xcEzLdsb_7ZNVe/_buildManifest.js" defer=""></script><script src="/_next/static/JnDgV02xcEzLdsb_7ZNVe/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main><p class="font-mono text-xl pt-4 text-center font-bold">Pytorch Advanced Part-1 DataParallel</p><p class="font-mono text-center text-s">2023-07</p><p class="font-mono text-center text-s">Rahul Chand</p><p class="font-mono text-center text-sm">------------</p><div class="Home_markdown___3bxt"><h3>DataParallel</h3>
<ol>
<li>Only one processes but multi threaded</li>
<li>In the forward pass, module is replicated on each device. During the backwards pass, gradients from each replica are summed into the original module.</li>
<li>The module is &quot;replicated&quot; &amp; then &quot;destroyed&quot; so a variable like <code class="">self.count</code> which is incremented each iteration will remain 0. If you want such a variable to change state define a mutable object like <code class="">List</code> or <code class="">Tensor</code></li>
</ol>
<h4>Specifics</h4>
<ol>
<li>You have to put your model onto the <strong>first</strong> gpu id you mention in <code class="">device_ids</code></li>
</ol>
<blockquote>
<p>Below example won&#x27;t work since model was placed on cuda:1 but first device id specified in DataParallel was 0</p>
</blockquote>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>model </span><span class="token token" style="color:#89ddff">=</span><span> Net</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>model </span><span class="token token" style="color:#89ddff">=</span><span> model</span><span class="token token" style="color:#89ddff">.</span><span>to</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#a5e844">&quot;cuda:1&quot;</span><span class="token token" style="color:#89ddff">)</span><span>
</span>
<span>net </span><span class="token token" style="color:#89ddff">=</span><span> torch</span><span class="token token" style="color:#89ddff">.</span><span>nn</span><span class="token token" style="color:#89ddff">.</span><span>DataParallel</span><span class="token token" style="color:#89ddff">(</span><span>model</span><span class="token token" style="color:#89ddff">,</span><span> device_ids</span><span class="token token" style="color:#89ddff">=</span><span class="token token" style="color:#89ddff">[</span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">,</span><span> output_device</span><span class="token token" style="color:#89ddff">=</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">)</span></code></div></pre>
<blockquote>
<p>But this example will work</p>
</blockquote>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>
</span><span>model </span><span class="token token" style="color:#89ddff">=</span><span> Net</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>model </span><span class="token token" style="color:#89ddff">=</span><span> model</span><span class="token token" style="color:#89ddff">.</span><span>to</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#a5e844">&quot;cuda:2&quot;</span><span class="token token" style="color:#89ddff">)</span><span>
</span>
<span>net </span><span class="token token" style="color:#89ddff">=</span><span> torch</span><span class="token token" style="color:#89ddff">.</span><span>nn</span><span class="token token" style="color:#89ddff">.</span><span>DataParallel</span><span class="token token" style="color:#89ddff">(</span><span>model</span><span class="token token" style="color:#89ddff">,</span><span> device_ids</span><span class="token token" style="color:#89ddff">=</span><span class="token token" style="color:#89ddff">[</span><span class="token token" style="color:#fd9170">2</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#fd9170">0</span><span class="token token" style="color:#89ddff">,</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">]</span><span class="token token" style="color:#89ddff">,</span><span> output_device</span><span class="token token" style="color:#89ddff">=</span><span class="token token" style="color:#fd9170">1</span><span class="token token" style="color:#89ddff">)</span><span>
</span></code></div></pre>
<ol start="2">
<li>The output will be of shape <code class="">(full_batch, ...)</code> if not a scalar. And a 1D tensor of shape <code class="">num_gpus</code> if scalar (e.g. if we directly return a loss)</li>
</ol>
<h4>How does .forward(...) work?</h4>
<ol>
<li>The base model (on the first gpu) is replicated and input is <code class="">scattered</code> and <code class="">parallel_apply</code> is called</li>
<li>Forward is not that interesting</li>
</ol>
<h4>How does .backward(...) work?</h4>
<ol>
<li>Lets say the output is a loss scalar</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#616161"># Lets say we have 4 GPUS</span><span>
</span><span>net </span><span class="token token" style="color:#89ddff">=</span><span> torch</span><span class="token token" style="color:#89ddff">.</span><span>nn</span><span class="token token" style="color:#89ddff">.</span><span>DataParallel</span><span class="token token" style="color:#89ddff">(</span><span>model</span><span class="token token" style="color:#89ddff">,</span><span> </span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#89ddff">.</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>out </span><span class="token token" style="color:#89ddff">=</span><span> net</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#ffcb6b">input</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span></span><span class="token token" style="color:#616161">#print(out.shape) -&gt; will print (4,) and tensor will be on device 0</span><span>
</span><span>out </span><span class="token token" style="color:#89ddff">=</span><span> torch</span><span class="token token" style="color:#89ddff">.</span><span>mean</span><span class="token token" style="color:#89ddff">(</span><span>out</span><span class="token token" style="color:#89ddff">)</span><span> </span><span class="token token" style="color:#616161">#We have to make it a scalar value before calling .backward() on it</span></code></div></pre>
<ol start="2">
<li>When we  call <code class="">out.backward()</code> even though it was on <code class="">cuda:0</code> it gets split again across devices and gradient is calculated w.r.t to each replica and then averaged and stored in the <code class="">cuda:0</code> instance</li>
</ol>
<h5>Tricky details</h5>
<ol>
<li>After each forward pass, only the values/tensors relevant to backward computation are stored. In below example <code class="">self.counter</code> variable state not be updated</li>
</ol>
<pre><div style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;overflow:auto;position:relative;margin:0.5em 0;padding:1.25em 1em"><code class="language-python" style="text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;color:#eee;background:#2f2f2f;font-family:Roboto Mono, monospace;font-size:1em;line-height:1.5em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#c792ea">class</span><span> </span><span class="token token" style="color:#f2ff00">MyModule</span><span class="token token" style="color:#89ddff">(</span><span>nn</span><span class="token token" style="color:#89ddff">.</span><span>Module</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">:</span><span>
</span>
<span>	</span><span class="token token" style="color:#c792ea">def</span><span> </span><span class="token token" style="color:#c792ea">__init__</span><span class="token token" style="color:#89ddff">(</span><span>self</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">:</span><span>
</span>
<span>		</span><span class="token token" style="color:#ffcb6b">super</span><span class="token token" style="color:#89ddff">(</span><span>MyModule</span><span class="token token" style="color:#89ddff">,</span><span> self</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">.</span><span>__init__</span><span class="token token" style="color:#89ddff">(</span><span class="token token" style="color:#89ddff">)</span><span>
</span><span>		self</span><span class="token token" style="color:#89ddff">.</span><span>counter </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#fd9170">0</span><span>
</span><span>		self</span><span class="token token" style="color:#89ddff">.</span><span>counter</span><span class="token token" style="color:#89ddff">.</span><span>requires_grad </span><span class="token token" style="color:#89ddff">=</span><span> </span><span class="token token" style="color:#c792ea">False</span><span>
</span>  
<span>	</span><span class="token token" style="color:#c792ea">def</span><span> </span><span class="token token" style="color:#c792ea">forward</span><span class="token token" style="color:#89ddff">(</span><span>self</span><span class="token token" style="color:#89ddff">,</span><span> x</span><span class="token token" style="color:#89ddff">)</span><span class="token token" style="color:#89ddff">:</span><span>
</span>
<span>		self</span><span class="token token" style="color:#89ddff">.</span><span>counter </span><span class="token token" style="color:#89ddff">+=</span><span> </span><span class="token token" style="color:#fd9170">1</span><span>
</span><span>		</span><span class="token token" style="color:#c792ea">return</span><span> x
</span></code></div></pre>
<ol start="2">
<li>DP is slower than DPP for one node multi GPU case because of GIL thread(?) and input scattering and output gathering.</li>
</ol></div><div class="Home_main3__oLarL"></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"dataparallel","contentHtml":"\n\n### DataParallel\n\n1. Only one processes but multi threaded\n2. In the forward pass, module is replicated on each device. During the backwards pass, gradients from each replica are summed into the original module.\n3. The module is \"replicated\" \u0026 then \"destroyed\" so a variable like `self.count` which is incremented each iteration will remain 0. If you want such a variable to change state define a mutable object like `List` or `Tensor`\n\n#### Specifics\n1. You have to put your model onto the **first** gpu id you mention in `device_ids`\n\u003e Below example won't work since model was placed on cuda:1 but first device id specified in DataParallel was 0\n\n\n```python\nmodel = Net()\nmodel = model.to(\"cuda:1\")\n\nnet = torch.nn.DataParallel(model, device_ids=[0,1,2], output_device=1)\n```\n\n\u003e But this example will work\n\n```python\n\nmodel = Net()\nmodel = model.to(\"cuda:2\")\n\nnet = torch.nn.DataParallel(model, device_ids=[2,0,1], output_device=1)\n\n```\n\n\n2. The output will be of shape `(full_batch, ...)` if not a scalar. And a 1D tensor of shape `num_gpus` if scalar (e.g. if we directly return a loss)\n\n\n#### How does .forward(...) work?\n\n1. The base model (on the first gpu) is replicated and input is `scattered` and `parallel_apply` is called\n2. Forward is not that interesting\n\n#### How does .backward(...) work?\n\n1. Lets say the output is a loss scalar\n```python\n# Lets say we have 4 GPUS\nnet = torch.nn.DataParallel(model, ......)\nout = net(input)\n#print(out.shape) -\u003e will print (4,) and tensor will be on device 0\nout = torch.mean(out) #We have to make it a scalar value before calling .backward() on it\n```\n2. When we  call `out.backward()` even though it was on `cuda:0` it gets split again across devices and gradient is calculated w.r.t to each replica and then averaged and stored in the `cuda:0` instance\n\n\n\n##### Tricky details\n1. After each forward pass, only the values/tensors relevant to backward computation are stored. In below example `self.counter` variable state not be updated\n\n```python\nclass MyModule(nn.Module):\n\n\tdef __init__(self):\n\n\t\tsuper(MyModule, self).__init__()\n\t\tself.counter = 0\n\t\tself.counter.requires_grad = False\n  \n\tdef forward(self, x):\n\n\t\tself.counter += 1\n\t\treturn x\n\n```\n\n\n2. DP is slower than DPP for one node multi GPU case because of GIL thread(?) and input scattering and output gathering. ","title":"Pytorch Advanced Part-1 DataParallel","date":"2023-07","name":"Rahul Chand"}},"__N_SSG":true},"page":"/blogs/pages/posts/[id]","query":{"id":"dataparallel"},"buildId":"JnDgV02xcEzLdsb_7ZNVe","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>