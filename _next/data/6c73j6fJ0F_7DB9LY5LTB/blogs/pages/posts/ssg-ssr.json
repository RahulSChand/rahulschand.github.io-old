{"pageProps":{"postData":{"id":"ssg-ssr","contentHtml":"\n\n### DataParallel\n\n1. Only one processes but multi threaded\n2. In the forward pass, module is replicated on each device. During the backwards pass, gradients from each replica are summed into the original module.\n3. The module is \"replicated\" & then \"destroyed\" so a variable like `self.count` which is incremented each iteration will remain 0. If you want such a variable to change state define a mutable object like `List` or `Tensor`\n\n#### Specifics\n1. You have to put your model onto the **first** gpu id you mention in `device_ids`\n> Below example won't work since model was placed on cuda:1 but first device id specified in DataParallel was 0\n\n\n```python\nmodel = Net()\nmodel = model.to(\"cuda:1\")\n\nnet = torch.nn.DataParallel(model, device_ids=[0,1,2], output_device=1)\n```\n\n> But this example will work\n\n```python\n\nmodel = Net()\nmodel = model.to(\"cuda:2\")\n\nnet = torch.nn.DataParallel(model, device_ids=[2,0,1], output_device=1)\n\n```\n\n\n2. The output will be of shape `(full_batch, ...)` if not a scalar. And a 1D tensor of shape `num_gpus` if scalar (e.g. if we directly return a loss)\n\n\n#### How does .forward(...) work?\n\n1. The base model (on the first gpu) is replicated and input is `scattered` and `parallel_apply` is called\n2. Forward is not that interesting\n\n#### How does .backward(...) work?\n\n1. Lets say the output is a loss scalar\n```python\n# Lets say we have 4 GPUS\nnet = torch.nn.DataParallel(model, ......)\nout = net(input)\n#print(out.shape) -> will print (4,) and tensor will be on device 0\nout = torch.mean(out) #We have to make it a scalar value before calling .backward() on it\n```\n2. When we  call `out.backward()` even though it was on `cuda:0` it gets split again across devices and gradient is calculated w.r.t to each replica and then averaged and stored in the `cuda:0` instance\n\n\n\n##### Tricky details\n1. After each forward pass, only the values/tensors relevant to backward computation are stored. In below example `self.counter` variable state not be updated\n\n```python\nclass MyModule(nn.Module):\n\n\tdef __init__(self):\n\n\t\tsuper(MyModule, self).__init__()\n\t\tself.counter = 0\n\t\tself.counter.requires_grad = False\n  \n\tdef forward(self, x):\n\n\t\tself.counter += 1\n\t\treturn x\n\n```\n\n\n2. DP is slower than DPP for one node multi GPU case because of GIL thread(?) and input scattering and output gathering. ","title":"When to Use Static Generation v.s. Server-side Rendering","date":"2022-01-02","name":"Rahul Chand"}},"__N_SSG":true}