{"pageProps":{"postData":{"id":"customBackward","contentHtml":"\n1. Custom backward : Two ways to write custom backward in PyTorch. \n\n### 1. Define a nn.Module with a backward and forward function in a class and use the *.backward()*\n\n```python\nimport torch\nclass MyCustomFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input):\n        # Perform forward computation\n        ctx.save_for_backward(input)\n        output = input * 2\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Perform backward computation\n        input, = ctx.saved_tensors\n        grad_input = grad_output * 2\n        return grad_input\n```\n\n- We use @staticmethod because no instantiation is required. \n- ctx is used to save all values that will be used in backward. These will be saved as part of the computation graph\n\n### 2. Define a class with a normal forward & a backward where you manually set the gradients\n\n```python\n\nclass MyCustomFunction(nn.Module):\n\tdef __init__(self):\n\t\tself.weight = nn.Linear(4,5)\n\n\tdef forward(self, x):\n\t\treturn self.weight(x)\n\t\n\tdef backward(self, input):\n\t\tself.weight.grad = input\n\t\treturn grad_out\n\t\t\n```\n\n- In this case we will call this class inside a `torch.no_grad()` and whatever gradient is returned from it will be used to call the natural `.backward(grad_out)`","title":"Writing Custom backward in PyTorch","date":"2023-08","name":"Rahul Chand"}},"__N_SSG":true}