{"pageProps":{"postData":{"id":"llama2_dummies","contentHtml":"\n# llama2.c for Dummies\n\n### Purpose\nThis repo is line by line walk through of the inference file in [llama2.c](http://github.com/karpathy/llama2.c). Its very verbose & intended for beginners.\n\nYou will need some familiarity with transformers architecture. If you are a complete novice refer to this excellent [blog](https://jalammar.github.io/illustrated-transformer/) first.\n\n----\n\n### Prerequisites\n\n1. Transformer architecture: 3 components\n\t1. Embedding (1 matmul)\n\t2. Layers: matmul with Q, K , V, O and feed forward weights: W1, W2 & W3. (7 matmul)\n\t3. Classifier: In our case the classifier is just matmul of `(vocab,768) x (768,1)` . Basically giving us what is the probability of each next token. (1 matmul)\n\n\n![Architechure ](https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/arch.png)\n\n\n\n## Code walkthrough\n\nCode has 3 parts, structs, functions & read logic in `main()` we will take a look at structs first, then go to main() and then cover the important functions.\n\n**PS: The code was taken from commit 4e23ad83. The original repo might be different as it gets newer commits.** But 99% of the logic should remain the same :) \n\n### Part 1: Structs\n\nWe define 3 structs for storing model config, model weights & to store intermediate values (run state) during forward pass\n\n1. **Config struct**: Defines the transformer model.\n\t1. `n_layers` , `vocab_size`  : no. of layers (e.g. llama-2 has 32 layers/BERT-base has 12 layers) & no. of tokens in our vocabulary (this is usually 30k for english languages)\n\t2. `dim` and `hidden_dim` : Define shape of Q, K, V & O `(dim,dim)` and W1, W2 `(dim, hidden_dim)`& W3 `(hidden_dim, dim)` \n\t3. `n_heads` : Number of heads for query(Q). If `n_heads=12` then matrix `Q=(768,768)` behaves/viewed as `(768, 768/12,768)`\n\t4. `n_kv_heads` : Number of heads for K & V. **Why are these different from above?** : Read [multi query paper](https://arxiv.org/pdf/1911.02150.pdf)\n\t5. `seq_len` : No. of tokens we will generate\n```c\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n```\n\n---\n\n2. **Weight struct** for llama. This is our pytorch `ffn=nn.Linear(...)` counterpart. \n\t1. Why are they `float*`  ? Because all matrices are just 1d flattened array. See below diagram\n\t2. code is self explanatory with shapes commented.   `rms_`  are weights used for normalization & `freq_cis_` are for [RoPE embedding](https://arxiv.org/pdf/2104.09864.pdf). We will look at `RoPE` in detail ahead.\n\t3. `wcls` is the final classifier. Matrix of size `(vocab, dim)` that maps final embedding from a vector to probability for each token in vocab.\n```c\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls\n    float* wq; // (layer, dim, dim)\n    float* wk; // (layer, dim, dim)\n    float* wv; // (layer, dim, dim)\n    float* wo; // (layer, dim, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // freq_cis for RoPE relatively positional embeddings\n    float* freq_cis_real; // (seq_len, dim/2)\n    float* freq_cis_imag; // (seq_len, dim/2)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n```\n\n![Array](https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/arr.png)\n\n---\n\n3. Intermediate activations (Run state)\n\t1. During forward pass we need to store intermediate values, e.g. output of matmul or output after norm. Will take a look at all variables later\n\t2. `key_cahce` and `value_cache` store the key, value outputs of previous tokens. e.g. during inference if the 5th token is being generated, this will store `key`, `value` of the previous 4.\n\n```c\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n```\n\n---\n\nWe will take a look at functions as we encounter them. For now lets see the logic inside `main()` \n\n### Part 2: Main (Can skip this part if you are only interested in [forward logic](#actual-forward-pass) )\n\n1. Get command line arguments. Nothing interesting.  Currently you can call `run.c` with\n\t1. `./run llama2_7b.bin`\n\t2. `./run llama2_7b.bin 0.1` -> with temperature\n\t3. `./run llama2_7b.bin 0.1 100` -> with temperature & steps (no. of output tokens generated)\n \n2. Declare `config` & `weights` in the end\n```c\nint main(int argc, char *argv[]) {\n    // poor man's C argparse\n    char *checkpoint = NULL;  // e.g. out/model.bin\n    float temperature = 0.9f; // e.g. 1.0, or 0.0\n    int steps = 256;          // max number of steps to run for, 0: use seq_len\n    // 'checkpoint' is necessary arg\n    if (argc < 2) {\n        printf(\"Usage: %s <checkpoint_file> [temperature] [steps]\\n\", argv[0]);\n        return 1;\n    }\n    if (argc >= 2) {\n        checkpoint = argv[1];\n    }\n    if (argc >= 3) {\n        // optional temperature. 0.0 = (deterministic) argmax sampling. 1.0 = baseline\n        temperature = atof(argv[2]);\n    }\n    if (argc >= 4) {\n        steps = atoi(argv[3]);\n    }\n\t// seed rng with time. if you want deterministic behavior use temperature 0.0\n    srand((unsigned int)time(NULL)); \n    // read in the model.bin file\n    Config config;\n    TransformerWeights weights;\n```\n\n2. Reading `checkpoint` file. \n\t1. If you are familiar with PyTorch. Usually  `config.json` & `model.bin` are separate (we load weights like a dictionary). But here `train.py` saves everything in one `.bin`  file in a specific format. This specific format allows us to easily read config & then each weight one by one.\n\n\tDetails\n\t1.  `shared_weights` : Should input embedding matrix & output classifier matrix be same?  \n\t2. Next load into `weights`. Get file size via `file_size = ftell(file);` Unlike vanilla PyTorch inference we **don't** load all weights into RAM. Instead we call `mmap(..)` to allocate RAM memory when we want lazily. For more detail [read](https://stackoverflow.com/questions/5877797/how-does-mmap-work)\n\t3. Finally  call `checkpoint_init_weights`  (snippet of function below). Here we map our weight pointers to correct address returned by `mmap`. Since we already read config we offset for it in line  `float* weights_ptr = data + sizeof(Config)/sizeof(float);`\n```c\nvoid checkpoint_init_weights(TransformerWeights *w, Config* p, float* f, int shared_weights){\nfloat* ptr = f;\nw->token_embedding_table = ptr;\nptr += p->vocab_size * p->dim;\nw->rms_att_weight = ptr;\n.......\n}\n```\n\nOriginal code we are talking about in above section\n```c\n    int fd = 0;\n    float* data = NULL;\n    long file_size;\n    {\n        FILE *file = fopen(checkpoint, \"rb\");\n        if (!file) {\n            printf(\"Unable to open the checkpoint file %s!\\n\", checkpoint);\n            return 1;\n        } \n\t    // read in the config header\n        if(fread(&config, sizeof(Config), 1, file) != 1) { return 1; }\n        // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n        int shared_weights = config.vocab_size > 0 ? 1 : 0;\n        config.vocab_size = abs(config.vocab_size);\n        // figure out the file size\n        fseek(file, 0, SEEK_END); // move file pointer to end of file\n        file_size = ftell(file); // get the file size, in bytes\n        fclose(file);\n        \n        // memory map the Transformer weights into the data pointer\n        fd = open(checkpoint, O_RDONLY); // open in read only mode\n        if (fd == -1) { printf(\"open failed!\\n\"); return 1; }\n        data = mmap(NULL, file_size, PROT_READ, MAP_PRIVATE, fd, 0);\n        if (data == MAP_FAILED) { printf(\"mmap failed!\\n\"); return 1; }\n        float* weights_ptr = data + sizeof(Config)/sizeof(float);\n        checkpoint_init_weights(&weights, &config, weights_ptr, shared_weights);\n    }\n```\n\n---\n\n3. Reading vocab file -> Mostly straightforward, only few details\n\t1. `vocab` is `char**` since each token is a string & `vocab` is a list of tokens.\n\t2. For loop over `vocab_size` & read each token\n```c\n// right now we cannot run for more than config.seq_len steps\n    if (steps <= 0 || steps > config.seq_len) { steps = config.seq_len; }\n    // read in the tokenizer.bin file\n    char** vocab = (char**)malloc(config.vocab_size * sizeof(char*));\n    {\n        FILE *file = fopen(\"tokenizer.bin\", \"rb\");\n        if (!file) {\n            printf(\"Unable to open the tokenizer file tokenizer.bin! Run \"\n            \"python tokenizer.py to convert tokenizer.model -> tokenizer.bin\\n\");\n            return 1;\n        }\n        int len;\n        for (int i = 0; i < config.vocab_size; i++) {\n            if(fread(&len, sizeof(int), 1, file) != 1) { return 1; }\n            vocab[i] = (char *)malloc(len + 1);\n            if(fread(vocab[i], len, 1, file) != 1) { return 1; }\n            vocab[i][len] = '\\0'; // add the string terminating token\n        }\n        fclose(file);\n    }\n```\n\n---\n\n#### Forward Loop & sampling in main (Go to [important part](#actual-forward-pass))\n\n1. Allocate memory for run state/intermediate values. The first `token` we pass into our model is BOS token (\"Beginning of Statement\") who's vocab index is `1`. \n```c\n    RunState state;\n    malloc_run_state(&state, &config);\n    \n    // the current position we are in\n    long start = time_in_ms();\n    int next;\n    int token = 1; // 1 = BOS token in Llama-2 sentencepiece\n    int pos = 0;\n    printf(\"<s>\\n\"); // explicit print the initial BOS token (=1), stylistically symmetric\n```\n\n![Luke](https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs//luke.jpeg)\n\n2. Forward loop:\n\t1. `transformer(token, pos, &config, &state, &weights);` stores classifier score of each token as being the next token in sequence inside `state.logits`.(contents of `transformer` function convered in next section). \n\t2. Next we sample. **Why we need sampling & how to do it?**\n \t\t- Lets say you want AI to complete dialogues of a movie & your input is _\"Luke, I am your\"_ . Now `llama` gives you score for each token to be the next word. So e.g. assume our tokens are `[\"Apple\", \"Football\", \"Father\", \"Brother\"]` & llama gives them scores of `[0.3, 0.1, 0.9, 0.7]`. Now to pick the next token, either we take maximum (`\"Father\"` with score 0.9) or we sample tokens with a probability proportional to thier score, this way we can get more diversity(very important in today's world ðŸ˜) in our prediction. \n\n \t3. Lets discuss some more details: If `temperature=0` then its max sampling.  For `temperate>0` we convert `state.logits` into probabilities using softmax & store back in `state.logits`. The `sample(..)` function returns a token sampled from the `state.logits` probability distribution. Read more [here](https://web.mit.edu/urban_or_book/www/book/chapter7/7.1.3.html) \n\t5. The token generated `next` becomes the next input token in line `token=next`. \n```c\nwhile (pos < steps) {\n        // forward the transformer to get logits for the next token\n        transformer(token, pos, &config, &state, &weights);\n        // sample the next token\n        if(temperature == 0.0f) {\n            // greedy argmax sampling\n            next = argmax(state.logits, config.vocab_size);\n        } else {\n            // apply the temperature to the logits\n            for (int q=0; q<config.vocab_size; q++) { state.logits[q] /= temperature; }\n            // apply softmax to the logits to get the probabilities for next token\n            softmax(state.logits, config.vocab_size);\n            // we now want to sample from this distribution to get the next token\n            next = sample(state.logits, config.vocab_size);\n        }\n        printf(\"%s\", vocab[next]);\n        fflush(stdout);\n\n        // advance forward\n        token = next;\n        pos++;\n    }\n```\n---\n\n### Actual Forward pass\n\nDetails of `transformer(token, pos, &config, &state, &weights);` called from `main()`\n\nSection below uses 2d/3d array indexing extensively. We cover it briefly here to make life easier\n\n1. If matrix `float* mat` is of size `(dim1, dim2, dim3)` then pointer to access `mat[l][i][j]` is `dim2*dim3*l + dim3*i + j;` -  This is `formula-1` we will refer to this often later. Read [link](https://www.learncpp.com/cpp-tutorial/pointer-arithmetic-and-array-indexing/) if you are confused\n\nHow to view matrices in terms of head?\n1.  K (key) `float* wk` is a matrix defined as shape `(layer, dim, dim)` when viewed in terms of heads is `(layer, dim, n_heads, head_dim)`\n---\n\n1. Convenience variables. Nothing interesting apart from copying the embedding of `token` into `s->xb` using `memcpy`. Why not use `float* content_row` itself? Because  `s->xb` is going to change & using `content_row` will change model weights.\n```c\nvoid transformer(int token, int pos, Config* p, RunState* s, TransformerWeights* w) {\n    // a few convenience variables\n    float *x = s->x;\n    int dim = p->dim;                  \n    int hidden_dim =  p->hidden_dim;  \n    int head_size = dim / p->n_heads; \n    float* content_row = &(w->token_embedding_table[token * dim]);\n    // copy the token embedding into x\n    memcpy(x, content_row, dim*sizeof(*x)); \n```\n\n---\n**RoPE** : Rotary Positional Embeddings \n- Formulation:  Transforms feature pairs by rotating it in 2D plane.\n\te.g. If your vector is `[0.8, 0.5, -0.1, 0.3]` we group them into pairs: `[[0.8,-0.1], [0.5, 0.3]` and rotate by some angle $\\theta$. This $\\theta$ is ~~part of the weights & is learned during training~~ $\\theta$ is fixed from the start (its not learnable). In the paper the value of $\\theta_{i}$ is $10000^{2(i-1)/d}$ \n\nRoPE  Formula (For 2 features grouped into a pair) is below. $m$ is the index of the pair. $\\theta$ is a learned parameter that we load from `.bin` file\n\n$$\n  \\left[ {\\begin{array}{ccccc}\n   x_{m}^{i} & x_{m}^{j} \\\\\n  \\end{array} } \\right] * \\left[ {\\begin{array}{ccccc}\n   cos(m\\theta_{m}) & -sin(m\\theta_{m}) \\\\\n   sin(m\\theta_{m}) & cos(m\\theta_{m}) \\\\\n  \\end{array} } \\right]\n$$\n\nOur example pair `[[0.8,-0.1], [0.5, 0.3]`  will be transformed like below. Keep in mind for the first pair `[0.8, 0.1]` $m=0$ since (therefore $sin(0)=0$). And for 2nd pair `m=1`\n\n$$\n  \\left[ {\\begin{array}{ccccc}\n   0.8 & -0.1 \\\\\n  \\end{array} } \\right] * \\left[ {\\begin{array}{ccccc}\n   1 * 1 & -0.0 * 1 \\\\\n   0.0 * 1 & 1.0 * 1 \\\\\n  \\end{array} } \\right] =    \\left[ {\\begin{array}{ccccc}\n   0.8 & -0.1 \\\\\n  \\end{array} } \\right]\n$$\n\n$$\n  \\left[ {\\begin{array}{ccccc}\n   0.5 & 0.3 \\\\\n  \\end{array} } \\right] * \\left[ {\\begin{array}{ccccc}\n   0.86 * 1 & -0.5 * 1 \\\\\n   0.5 * 1 & 0.86 * 1 \\\\\n  \\end{array} } \\right] =    \\left[ {\\begin{array}{ccccc}\n   0.58 & 0.08 \\\\\n  \\end{array} } \\right]\n$$\n\nCombining both, the output is `[[0.8, 0.1], [0.58, 0.08]]` now **un-pairing** them will give us `[0.8, 0.58, 0.1, 0.08]`\nSo `RoPE` transformed `[0.8, 0.5, -0.1, 0.3]` into `[0.8, 0.58, 0.1, 0.08]`. Keep in mind if a feature is of `dim=768` then there are half of it **384** learnable $\\theta$'s. \n\n**Back to code**\n1. We get $\\theta$ for current position (`pos` is our $m$).  `freq_cis_real_row` is $cos(m\\theta)$ and `freq_cis_imag_row` is $sin(m\\theta)$.\n```c\n    // pluck out the \"pos\" row of freq_cis_real and freq_cis_imag66\n    float* freq_cis_real_row = w->freq_cis_real + pos * head_size / 2;\n    float* freq_cis_imag_row = w->freq_cis_imag + pos * head_size / 2;\n```\n\n2. Iterate over layers. Apply `rmsnorm` to input of the layer.  `rmsnorm` function calculates the below\n\n```math\nout\\; = \\;  (x*g*n)/\\sum_{i} \\sqrt{x_{i}^{2}} \n```\nwhere $x$ is input, $g$ is learnable parameter (`w->rms_attn_weight` below) & $n$ is `dim`.\n\n`matmul` does matrix mult of a 2d matrix with a 1d matrix. `(A, B) x (A,)`. The implementation is trivial (we cover this at very end). We multiply Q,K,V with `s->xb` (output of `rmsnorm`) and store output in `s->q`, `s->k` ..\n```c\nfor(int l = 0; l < p->n_layers; l++) {\n// attention rmsnorm\n\trmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\t\n\t// qkv matmuls for this position\n\tmatmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n\tmatmul(s->k, s->xb, w->wk + l*dim*dim, dim, dim);\n\tmatmul(s->v, s->xb, w->wv + l*dim*dim, dim, dim);\n```\n3. Go over each head & apply the 2-d $cos$/$sin$ transformation we discussed above to `s->q` and `s->k`. We do it separately for each head, therefore we take offset of `h*head_size`\n```c\n// apply RoPE rotation to the q and k vectors for each head\n        for (int h = 0; h < p->n_heads; h++) {\n            // get the q and k vectors for this head\n            float* q = s->q + h * head_size;\n            float* k = s->k + h * head_size;\n            // rotate q and k by the freq_cis_real and freq_cis_imag\n            for (int i = 0; i < head_size; i+=2) {\n                float q0 = q[i];\n                float q1 = q[i+1];\n                float k0 = k[i];\n                float k1 = k[i+1];\n                float fcr = freq_cis_real_row[i/2];\n                float fci = freq_cis_imag_row[i/2];\n                q[i]   = q0 * fcr - q1 * fci;\n                q[i+1] = q0 * fci + q1 * fcr;\n                k[i]   = k0 * fcr - k1 * fci;\n                k[i+1] = k0 * fci + k1 * fcr;\n            }\n        }\n```\n\n\n4. Once we get `q, k, v` for current token, we need to calculate self-attention. Where we multiply query into key.  `k & v` are only for the current token. We store the `k, v` for all past tokens in `key_cache_row`  & `value_cache_row`.\n\t- For example, if we have generated the tokens (\"fox\", \"jumps\", \"over\") until now then we already have Q & V for \"fox\" & \"jumps\" from previous forward passes stored in our cache. We need not recalculate.\n\t- Since caches store key, query for all layers & for all tokens (max no.of tokens is `seq_length`) its dimensions are `(layer, seq_length, dim)`. `seq_length` is usually called `context`. \n5. Consider below code in terms of above example. Lets say `seq_length=32` (which means we generate at-most 32 tokens). `pos=2` since \"fox\" is the 3rd token (2nd since python is 0-indexed). \n\t- We already have `layer*(pos-1)*dim` values filled in `s->key_cache` We need to fill the key, value of current token \"fox\" into `s->key_cache` too before doing self-attention. This is what `memcpy(key_cache_row, s->k, dim*sizeof(*key_cache_row));` does\n```c\n// save key,value at this time step (pos) to our kv cache\nint loff = l * p->seq_len * dim; // kv cache layer offset for convenience\nfloat* key_cache_row = s->key_cache + loff + pos * dim;\nfloat* value_cache_row = s->value_cache + loff + pos * dim;\nmemcpy(key_cache_row, s->k, dim*sizeof(*key_cache_row));\nmemcpy(value_cache_row, s->v, dim*sizeof(*value_cache_row));\n```\n\n### Doing self-attention\n\nFormula\n\n```math\n\\begin{align} \nout = (QK^{T})\\;V/\\sqrt{d} \\\\\nwhere\\;\\;\\; Q=(1,dim) \\;\\; K=(dim,N) \\;\\; V=(dim,N)\n\\end{align}\n```\nIn above $N$ is `pos` (current length of the generated text)\n\n\nThis part of the code becomes easy if you remember that `s->q`, `s->k` when viewed in terms of heads are of shape `(dim, n_heads, head_dim)` & `key_cache`'s are `(seq_length, n_heads, head_dim)`. Lets go over the code\n 1. `int h` is the current head count. Lets look at each line one by one\n\t 1. `q = s->q + h*head_size` :  Gets pointer to start of $h^{th}$ head. Remember `formula-1`. Matrix is of size `(dim, n_heads, head_dim)` we need `s->q[0][h][0]` which is `0*n_heads*head_dim + h*head_dim + 0` which is `h*head_size`. \n\t 2. `att = s->att + h * p->seq_len`: We will store attention in `s->attn` run state variable.\n\t 3. For each position (`pos` is 2 currently if you go back to \"fox\", \"jumps\", \"over\" example) \n\t\t\t 1.To get  $l^{th}$ layer, $t^{th}$ position & $h^{th}$ head we do `s->key_cache + l*seq_length*dim + t*n_heads*head_dim + h*head_dim` . Since `loff` defined before is already `l*seq_length*dim`. Final offset is `loff + t*n_heads*head_dim + h*head_size` since `n_heads*head_dim=dim` we get offset as `loff + t*dim + h*head_size`.\n\t1. We now have `q` `(head_size,)`, `k` `(head_size,)`  & `att` `(seq_length,)`. We can calculate self-attention score for $h^{th}$ head at position $t$.  We sum this over all the heads & positions till now.\n```c\n\tint h;        \n\t#pragma omp parallel for private(h)\n\tfor (h = 0; h < p->n_heads; h++) {\n\t// get the query vector for this head\n\tfloat* q = s->q + h * head_size;\n\t// attention scores for this head\n\tfloat* att = s->att + h * p->seq_len;\n\t// iterate over all timesteps, including the current one\n\tfor (int t = 0; t <= pos; t++) {\n\t\t// get the key vector for this head and at this timestep\n\t\tfloat* k = s->key_cache + loff + t * dim + h * head_size;\n\t\t// calculate the attention score as the dot product of q and k\n\t\tfloat score = 0.0f;\n\t\tfor (int i = 0; i < head_size; i++) {\n\t\t\tscore += q[i] * k[i];\n\t\t}\n\t\tscore /= sqrtf(head_size);\n\t\t// save the score to the attention buffer\n\t\tatt[t] = score;\n ```\n\n\n2. `attn` obtained above is of shape `(seq_length, )`. Next we multiply it with `v` which is `(seq_length, dim)`. Remember the below loop is inside the `for (h = 0; h < p->n_heads; h++)` that started in previous section.\n\n```c\n// softmax the scores to get attention weights, from 0..pos inclusively\nsoftmax(att, pos + 1);\n\n// weighted sum of the values, store back into xb\nfloat* xb = s->xb + h * head_size;\nmemset(xb, 0, head_size * sizeof(float));\nfor (int t = 0; t <= pos; t++) {\n\t// get the value vector for this head and at this timestep\n\tfloat* v = s->value_cache + loff + t * dim + h * head_size;\n\t// get the attention weight for this timestep\n\tfloat a = att[t];\n\t// accumulate the weighted value into xb\n\tfor (int i = 0; i < head_size; i++) {\n\t\txb[i] += a * v[i];\n\t}\n}\n```\n\n\n---\n\n### Feed Forward & Classifier\n\n1. To complete attention module, we need to multiply with $O$ which we do in first line. Next line `accum` adds input which comes from skip layer (red arrow) & output of attention. Followed by normalization.\n\n```c\n// final matmul to get the output of the attention\nmatmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n// residual connection back into x\naccum(x, s->xb2, dim);\n// ffn rmsnorm\nrmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n```\n![Accum](https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/accum.png)\n\n\n2. Next we calculate the FFN output which is\n```math\nout = W_{3}\\;\\sigma (W_{1}X*W_{2}X)\n```\n$\\sigma$ is `silu` [activation](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html). \n\n\n![Silu](https://pytorch.org/docs/stable/_images/SiLU.png)\n\n\nThis portion is self explanatory \n```c\n// Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n// first calculate self.w1(x) and self.w3(x)\nmatmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\nmatmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n// F.silu; silu(x)=x*Ïƒ(x),where Ïƒ(x) is the logistic sigmoid\nfor (int i = 0; i < hidden_dim; i++) {\n\ts->hb[i] = s->hb[i] * (1.0f / (1.0f + expf(-s->hb[i])));\n}\n// elementwise multiply with w3(x)\nfor (int i = 0; i < hidden_dim; i++) {\n\ts->hb[i] = s->hb[i] * s->hb2[i];\n}\n// final matmul to get the output of the ffn\n//memcpy(tmp_w_hid, w->w2 + l*dim*hidden_dim, hidden_dim*dim*sizeof(float));\nmatmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n```\n3. The last line is another accum (2nd skip layer in above diagram)\n```c\naccum(x, s->xb, dim);\n```\n\n\n---\n\n### Final Classifier\n\nAfter running above module for all layers, we get an embedding of shape `(dim,)`. We need to convert this into a vector of shape `(vocab,)` whose each entry tells us what is the score for that word to be next token.\n\n1. Before multiplying with classifier matrix (`w->wcls`) we normalize our embedding. The scores our saved in `s->logits`\n```c\n// final rmsnorm\nrmsnorm(x, x, w->rms_final_weight, dim);\n// classifier into logits\nmatmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n```\n\n---\n### The end\n\nOnce we get `s->logits` we sample next token (do this until we get `seq_length` tokens). This has already been covered in \"Forward Loop & sampling in main\" section.  Congratulations! now you know how LLMs work & how to code them in C. If you now want to know how to code them in Python know, refer to [modelling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)\n\nHere is a picture of a cat :)  \n\n![Cat](https://raw.githubusercontent.com/RahulSChand/llama2.c-for-dummies/main/imgs/cat.jpg)\n\n\n\n\n\n\n\n\n\n","title":"llama2.c for dummies - Introduction to llama2 in C (Taken directly form my github)","date":"2023-08","name":"Rahul Chand"}},"__N_SSG":true}